{
 "cells": [
  {
   "cell_type": "code",
   "id": "408334c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:55:32.013092Z",
     "start_time": "2025-05-23T16:55:31.975817Z"
    }
   },
   "source": [
    "# Understand the COCO dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "with open(f\"{project_root}/data/coco/subset/annotations/instances_subset.json\") as f:\n",
    "    coco = json.load(f)     # dict contains filenames and ground truth classes\n",
    "\n",
    "categories = coco[\"categories\"]\n",
    "\n",
    "for c in categories:\n",
    "    print(f\"Category ID: {c['id']} Name: {c['name']}\")\n",
    "# 1 - 90 categories\n",
    "print(f\"Number of Categories: {len(categories)}\")\n",
    "\n",
    "# Data structure\n",
    "BASE_DIR = \"data/coco/images/train2017\" if os.environ.get(\"USE_FULL_DATASET\", 0) == 1 else \"data/coco/subset/images\"\n",
    "print(f\"Segmentation: The coordinates for a polygon line of the detected object: {coco['annotations'][2]['segmentation']} \")\n",
    "print(f\"Area: The area of the detected object: {coco['annotations'][2]['area']}\")\n",
    "print(f\"Image ID: {coco['annotations'][2]['id']}\")\n",
    "print(f\"Number of annotations: {len(coco['annotations'])}\")     # 7000+, we need to filter 1K which really exist in our images folder\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category ID: 1 Name: person\n",
      "Category ID: 2 Name: bicycle\n",
      "Category ID: 3 Name: car\n",
      "Category ID: 4 Name: motorcycle\n",
      "Category ID: 5 Name: airplane\n",
      "Category ID: 6 Name: bus\n",
      "Category ID: 7 Name: train\n",
      "Category ID: 8 Name: truck\n",
      "Category ID: 9 Name: boat\n",
      "Category ID: 10 Name: traffic light\n",
      "Category ID: 11 Name: fire hydrant\n",
      "Category ID: 13 Name: stop sign\n",
      "Category ID: 14 Name: parking meter\n",
      "Category ID: 15 Name: bench\n",
      "Category ID: 16 Name: bird\n",
      "Category ID: 17 Name: cat\n",
      "Category ID: 18 Name: dog\n",
      "Category ID: 19 Name: horse\n",
      "Category ID: 20 Name: sheep\n",
      "Category ID: 21 Name: cow\n",
      "Category ID: 22 Name: elephant\n",
      "Category ID: 23 Name: bear\n",
      "Category ID: 24 Name: zebra\n",
      "Category ID: 25 Name: giraffe\n",
      "Category ID: 27 Name: backpack\n",
      "Category ID: 28 Name: umbrella\n",
      "Category ID: 31 Name: handbag\n",
      "Category ID: 32 Name: tie\n",
      "Category ID: 33 Name: suitcase\n",
      "Category ID: 34 Name: frisbee\n",
      "Category ID: 35 Name: skis\n",
      "Category ID: 36 Name: snowboard\n",
      "Category ID: 37 Name: sports ball\n",
      "Category ID: 38 Name: kite\n",
      "Category ID: 39 Name: baseball bat\n",
      "Category ID: 40 Name: baseball glove\n",
      "Category ID: 41 Name: skateboard\n",
      "Category ID: 42 Name: surfboard\n",
      "Category ID: 43 Name: tennis racket\n",
      "Category ID: 44 Name: bottle\n",
      "Category ID: 46 Name: wine glass\n",
      "Category ID: 47 Name: cup\n",
      "Category ID: 48 Name: fork\n",
      "Category ID: 49 Name: knife\n",
      "Category ID: 50 Name: spoon\n",
      "Category ID: 51 Name: bowl\n",
      "Category ID: 52 Name: banana\n",
      "Category ID: 53 Name: apple\n",
      "Category ID: 54 Name: sandwich\n",
      "Category ID: 55 Name: orange\n",
      "Category ID: 56 Name: broccoli\n",
      "Category ID: 57 Name: carrot\n",
      "Category ID: 58 Name: hot dog\n",
      "Category ID: 59 Name: pizza\n",
      "Category ID: 60 Name: donut\n",
      "Category ID: 61 Name: cake\n",
      "Category ID: 62 Name: chair\n",
      "Category ID: 63 Name: couch\n",
      "Category ID: 64 Name: potted plant\n",
      "Category ID: 65 Name: bed\n",
      "Category ID: 67 Name: dining table\n",
      "Category ID: 70 Name: toilet\n",
      "Category ID: 72 Name: tv\n",
      "Category ID: 73 Name: laptop\n",
      "Category ID: 74 Name: mouse\n",
      "Category ID: 75 Name: remote\n",
      "Category ID: 76 Name: keyboard\n",
      "Category ID: 77 Name: cell phone\n",
      "Category ID: 78 Name: microwave\n",
      "Category ID: 79 Name: oven\n",
      "Category ID: 80 Name: toaster\n",
      "Category ID: 81 Name: sink\n",
      "Category ID: 82 Name: refrigerator\n",
      "Category ID: 84 Name: book\n",
      "Category ID: 85 Name: clock\n",
      "Category ID: 86 Name: vase\n",
      "Category ID: 87 Name: scissors\n",
      "Category ID: 88 Name: teddy bear\n",
      "Category ID: 89 Name: hair drier\n",
      "Category ID: 90 Name: toothbrush\n",
      "Number of Categories: 80\n",
      "Segmentation: The coordinates for a polygon line of the detected object: [[295.16, 354.66, 295.16, 347.06, 290.6, 336.42, 292.12, 329.58, 292.88, 318.56, 295.92, 316.66, 303.91, 317.42, 318.73, 315.52, 320.63, 317.04, 325.57, 323.12, 322.91, 326.54, 320.25, 331.48, 316.83, 334.14, 313.03, 342.5, 312.27, 353.52, 312.27, 358.46, 294.78, 356.94]] \n",
      "Area: The area of the detected object: 983.4817999999985\n",
      "Image ID: 19837\n",
      "Number of annotations: 7462\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fee5666-0b96-49be-8bd1-6fed4a6fd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "import os, json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCO classification dataset.\n",
    "    (image_tensor, one_hot)    # multi-label, softmax, one image contains multiple objects\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_dir: str,\n",
    "                 ann_file: str,\n",
    "                 transform=None):\n",
    "        self.img_dir      = img_dir\n",
    "        self.transform    = transform\n",
    "\n",
    "        # parse COCO json\n",
    "        with open(ann_file, \"r\") as f:\n",
    "            coco = json.load(f)\n",
    "\n",
    "        self.cat_ids      = sorted({c[\"id\"] for c in coco[\"categories\"]})\n",
    "        self.idx2cat_id = {i: cid for i, cid in enumerate(self.cat_ids)}\n",
    "        self.cat_id2idx = {cid: i for i, cid in enumerate(self.cat_ids)}\n",
    "        self.num_classes  = len(self.cat_ids)\n",
    "\n",
    "        # map image id to related list of categories\n",
    "        img_to_cats = {}\n",
    "        for ann in coco[\"annotations\"]:\n",
    "            # {image_id: [category_11, category_23, ...]}\n",
    "            img_to_cats.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
    "\n",
    "        # keep only images that actually have annotations\n",
    "        # filenames: categories\n",
    "        self.samples = [\n",
    "            (img[\"file_name\"], img_to_cats[img[\"id\"]])\n",
    "            for img in coco[\"images\"]\n",
    "            if img[\"id\"] in img_to_cats\n",
    "        ]\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(\"No annotated images found!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, cats = self.samples[idx]\n",
    "        img_path    = os.path.join(self.img_dir, fname)\n",
    "        img         = Image.open(img_path).convert(\"RGB\")   # make sure only 3 channels\n",
    "\n",
    "        # preprocessing the image if there's predefined transform(size, etc.)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # multi-label one-hot vector, all zeros for initiation\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        # ground truth category of this image in one-hot encoding, mark those valid categories into 1\n",
    "        for cid in cats:\n",
    "            target[self.cat_id2idx[cid]] = 1.0\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d6334e-2a63-4d00-b883-6fc416da58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 990\n",
      "Sample Data: <PIL.Image.Image image mode=RGB size=640x501 at 0x119A6ADC0>\n",
      "Ground Truth Category:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Check ImageDataset\n",
    "image_ds = ImageDataset(img_dir=\"data/coco/subset/images\", ann_file=\"data/coco/subset/annotations/instances_subset.json\")\n",
    "print(f\"Data length: {len(image_ds)}\")\n",
    "print(f\"Sample Data: {image_ds[33][0]}\")\n",
    "print(f\"Ground Truth Category:\\n {image_ds[33][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff97ba2-d675-41f8-afc2-90fade32f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define backbone of ResNet18 model\n",
    "\n",
    "import torch.nn as nn  # Neural Network lib\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class MyResNet18(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18 backbone\n",
    "    Inherit PyTorch nn Module, define and train my own ResNet from scratch\n",
    "    num_classes: class num for the model, 91 classes(categories) for COCO dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1000, weights=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int):\n",
    "                Number of output channels from the final linear layer.\n",
    "                For ImageNet classification use 1000; for a custom task like COCO, 91 classes(categories)\n",
    "            weights (bool):\n",
    "                If True, loads ImageNet‑pretrained weights.\n",
    "                If False, train from scratch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # load ResNet18 architecture\n",
    "        self.model = torchvision.models.resnet18(\n",
    "            weights=weights)  # Main feature extractor, no pretrained weights\n",
    "        # replace the final fully connected layer with new classes\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, 3, H, W],\n",
    "                              where B = batch size.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [B, num_classes].\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd4ad25-789f-458d-ba0f-b3a46670aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform, image preprocessing\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((480, 480)),      # smaller -> trains faster\n",
    "    transforms.RandomHorizontalFlip(0.5),   # probability of flipping current image\n",
    "    transforms.ToTensor(),              # PIL -> PyTorch FloatTensor\n",
    "    transforms.Normalize(               # data from ImageNet, pixel normalization\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4acaf169-b005-485a-85ac-6ce026027152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Training related functions definitions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_dataset_paths():\n",
    "    \"\"\"\n",
    "    Get appropriate data paths based on environment variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image_directory_path, annotation_file_path)\n",
    "    \"\"\"\n",
    "    use_full = os.environ.get(\"USE_FULL_DATASET\", \"0\").lower() in (\"1\", \"true\", \"yes\")\n",
    "    base_dir = os.path.abspath(\".\")\n",
    "\n",
    "    if use_full:\n",
    "        img_dir = os.path.join(base_dir, \"data\", \"coco\", \"images\", \"train2017\")\n",
    "        ann_file = os.path.join(base_dir, \"data\", \"coco\", \"annotations\", \"instances_train2017.json\")\n",
    "    else:\n",
    "        img_dir = os.path.join(base_dir, \"data\", \"coco\", \"subset\", \"images\")\n",
    "        ann_file = os.path.join(base_dir, \"data\", \"coco\", \"subset\", \"annotations\", \"instances_subset.json\")\n",
    "\n",
    "    return img_dir, ann_file\n",
    "\n",
    "\n",
    "def select_device():\n",
    "    \"\"\"\n",
    "    Select the appropriate device for training.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The device to use for training\n",
    "    \"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def setup_mlflow(batch_size, learning_rate, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Set up and configure the MLFlow for experiment tracking.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Training batch size\n",
    "        learning_rate (float): Learning rate\n",
    "        num_epochs (int): Number of training epochs\n",
    "        device (torch.device): Training device\n",
    "\n",
    "    Returns:\n",
    "        MLFlow instance\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\"))\n",
    "    mlflow.set_experiment(\"ResNet18_COCO\")\n",
    "    mlflow_instance = mlflow.start_run(run_name=datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"model\": \"ResNet18\",\n",
    "        \"device\": device.type\n",
    "    })\n",
    "    return mlflow_instance\n",
    "\n",
    "\n",
    "\n",
    "def create_checkpoint_dir():\n",
    "    \"\"\"\n",
    "    Create directory for saving model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to checkpoint directory\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = os.path.join(\"checkpoints\", timestamp)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    logger.info(f\"Checkpoints will be saved to: {save_dir}\")\n",
    "    return save_dir\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, accuracy, save_dir):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to save\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to save\n",
    "        epoch (int): Current epoch number\n",
    "        accuracy (float): Current accuracy\n",
    "        save_dir (str): Directory to save checkpoint\n",
    "\n",
    "    Returns:\n",
    "        str: Path to saved checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint_path = os.path.join(save_dir, f\"resnet18_epoch_{epoch + 1}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'accuracy': accuracy\n",
    "    }, checkpoint_path)\n",
    "    logger.info(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def track_metrics(metrics_dict, epoch, step=None, context=None):\n",
    "    \"\"\"\n",
    "    Track multiple metrics in mlflow.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict (dict): Dictionary of metrics to track\n",
    "        epoch (int): Current epoch\n",
    "        step (int, optional): Current step within the epoch\n",
    "        context (dict, optional): Additional context for the metrics\n",
    "    \"\"\"\n",
    "    context = context or {\"subset\": \"train\"}\n",
    "\n",
    "    for name, value in metrics_dict.items():\n",
    "        if step is not None:\n",
    "            mlflow.log_metric(name, value, step=step)\n",
    "        else:\n",
    "            mlflow.log_metric(name, value, step=epoch)\n",
    "\n",
    "\n",
    "def calculate_metrics(all_targets, all_predictions):\n",
    "    \"\"\"\n",
    "    all_targets      - list of (C,) one-hot np.float32\n",
    "    all_predictions  - list of (C,) sigmoid scores np.float32\n",
    "    \"\"\"\n",
    "    y_true  = np.vstack(all_targets).astype(np.int8)     # (N, C)\n",
    "    y_score = np.vstack(all_predictions)                 # (N, C) ∈[0,1]\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Average precision (macro over classes that have ≥1 positive)\n",
    "    aps = []\n",
    "    for c in range(y_true.shape[1]):\n",
    "        if y_true[:, c].sum() == 0:      # skip empty class\n",
    "            continue\n",
    "        aps.append(average_precision_score(y_true[:, c], y_score[:, c]))\n",
    "    metrics[\"avg_precision\"] = float(np.mean(aps)) if aps else 0.0\n",
    "\n",
    "    # ROC AUC (macro over valid classes)\n",
    "    aucs = []\n",
    "    for c in range(y_true.shape[1]):\n",
    "        pos = y_true[:, c].sum()\n",
    "        neg = (1 - y_true[:, c]).sum()\n",
    "        if pos == 0 or neg == 0:\n",
    "            continue                     # undefined\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, c], y_score[:, c])\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "    metrics[\"roc_auc\"] = float(np.mean(aucs)) if aucs else 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Main logic for one training epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss   = 0.0\n",
    "    running_correct = 0\n",
    "    running_total   = 0\n",
    "    log_interval    = 10\n",
    "\n",
    "    all_targets      = []\n",
    "    all_predictions  = []\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images   = images.to(device)\n",
    "        targets  = targets.to(device).float()          # one-hot\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits     = model(images)\n",
    "        loss       = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy: label-wise (much more informative early)\n",
    "        probs = logits.sigmoid()\n",
    "        preds = (probs > 0.5)                                       # filter out all probabilities > 0.5 in the vector\n",
    "\n",
    "        correct_labels = (preds == targets.bool()).sum().item()     # see how many categories are correctly predicted\n",
    "        total_labels   = targets.numel()\n",
    "\n",
    "        running_correct += correct_labels\n",
    "        running_total   += total_labels\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Mlflow logging every batch\n",
    "        track_metrics(\n",
    "            {\"loss\": loss.item()},\n",
    "            epoch,\n",
    "            step=i\n",
    "        )\n",
    "\n",
    "        # Periodic console log\n",
    "        if i % log_interval == log_interval - 1:\n",
    "            avg_loss  = running_loss / log_interval\n",
    "            acc_sofar = running_correct / running_total\n",
    "            logger.info(f\"Epoch {epoch + 1} | Batch {i + 1} | Loss {avg_loss:.4f} | Accuracy {acc_sofar:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "        # save for end-of-epoch metrics\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        all_predictions.append(logits.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    epoch_accuracy = running_correct / running_total\n",
    "    all_targets    = np.vstack(all_targets)\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "\n",
    "    return epoch_accuracy, all_targets, all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dce4b1-2bd6-4986-8327-8b1e86a1a1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 23:10:54,028 - INFO - Using device: mps\n",
      "2025-05-21 23:10:54,047 - INFO - Checkpoints will be saved to: checkpoints/20250521_231054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with data from: /Users/yj/Workspaces/ResNet-CIFAR10/data/coco/subset/images\n",
      "Using annotations from: /Users/yj/Workspaces/ResNet-CIFAR10/data/coco/subset/annotations/instances_subset.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 23:11:18,616 - INFO - Epoch 1 | Batch 10 | Loss 0.6221 | Accuracy 0.6889\n"
     ]
    }
   ],
   "source": [
    "# main train logic\n",
    "from mlflow.models import infer_signature, validate_serving_input, convert_input_example_to_serving_input\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Set hyperparameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    # Setup device, mlflow tracking, and checkpoint directory\n",
    "    device = select_device()\n",
    "    mlflow_instance = setup_mlflow(batch_size, learning_rate, num_epochs, device)\n",
    "    save_dir = create_checkpoint_dir()\n",
    "\n",
    "    # Get dataset paths and setup data\n",
    "    img_dir, ann_file = get_dataset_paths()\n",
    "    print(f\"Training with data from: {img_dir}\")\n",
    "    print(f\"Using annotations from: {ann_file}\")\n",
    "\n",
    "    # Create datasets and dataloaders - disable pin_memory on MPS\n",
    "    train_dataset = ImageDataset(img_dir=img_dir, ann_file=ann_file, transform=transform)\n",
    "\n",
    "    # Disable pin_memory explicitly on MPS to avoid warnings\n",
    "    pin_memory = device.type != \"mps\"\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0 if device.type == \"mps\" else 4,       # 0 threads to avoid issue using mps on macos\n",
    "        pin_memory=pin_memory                               # Disable for MPS\n",
    "    )\n",
    "\n",
    "    # Initialize the model with weights=None instead of pretrained=False\n",
    "    model = MyResNet18(num_classes=train_dataset.num_classes, weights=None)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer, Multi-label for sigmoid + BCE\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_acc = float(\"-inf\")                     # use for telling best model so far\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train for one epoch\n",
    "        epoch_accuracy, all_targets, all_predictions = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device, epoch\n",
    "        )\n",
    "\n",
    "        if epoch_accuracy > best_acc:                # new best model found: persist storage + logging checkpoints\n",
    "            best_acc = epoch_accuracy\n",
    "            # reuse your helper\n",
    "            best_path = save_checkpoint(model, optimizer, epoch, best_acc, save_dir)\n",
    "            # put the raw .pt file in MLflow artifacts\n",
    "            mlflow.log_artifact(best_path, artifact_path=\"checkpoints\")\n",
    "            # package the model for serving / registry\n",
    "            torch_sample = torch.randn(1, 3, 224, 224, dtype=torch.float32)\n",
    "            np_sample    = torch_sample.cpu().numpy().astype(np.float32)\n",
    "            \n",
    "            # infer signature (input → output)\n",
    "            signature = infer_signature(\n",
    "                np_sample,\n",
    "                model(torch_sample.to(device)).detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            pip_reqs = [\n",
    "                f\"torch=={torch.__version__}\",\n",
    "                f\"torchvision=={torchvision.__version__}\",\n",
    "            ]\n",
    "\n",
    "            mlflow.pytorch.log_model(\n",
    "                model,\n",
    "                artifact_path=\"best_model\",\n",
    "                signature=signature,            # passes schema, no input_example needed\n",
    "                pip_requirements=pip_reqs\n",
    "            )\n",
    "\n",
    "        # Track epoch-level metrics\n",
    "        mlflow.log_metric(\"epoch_accuracy\", epoch_accuracy, step=epoch)\n",
    "        print(f'Epoch: {epoch + 1} completed, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "        # Track learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        mlflow.log_metric(\"learning_rate\", current_lr, step=epoch)\n",
    "\n",
    "        # Calculate and track model metrics\n",
    "        metrics = calculate_metrics(\n",
    "            all_targets, all_predictions\n",
    "        )\n",
    "\n",
    "        # Track overall average precision\n",
    "        mlflow.log_metric(\"avg_precision\", metrics[\"avg_precision\"], step=epoch)\n",
    "\n",
    "        # Track overall ROC AUC (macro-average across classes)\n",
    "        mlflow.log_metric(\"roc_auc\", metrics[\"roc_auc\"], step=epoch)\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(model, optimizer, epoch, epoch_accuracy, save_dir)\n",
    "\n",
    "    mlflow.end_run()\n",
    "        \n",
    "\n",
    "try:\n",
    "    train()                             #  train() already starts & ends the run\n",
    "except Exception as e:\n",
    "    mlflow.end_run(status=\"FAILED\")     # avoid RUNNING but failed task jam the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to end the failed test, avoid RUNNING but failed task jam the workflow\n",
    "mlflow.end_run(\"6ef3867a369b45f3ba0bc1e2e74a4d90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cf3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
