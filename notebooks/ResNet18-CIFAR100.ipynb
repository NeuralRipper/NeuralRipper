{
 "cells": [
  {
   "cell_type": "code",
   "id": "bff97ba2-d675-41f8-afc2-90fade32f0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:13:25.561785Z",
     "start_time": "2025-05-24T19:13:25.557112Z"
    }
   },
   "source": [
    "# Define backbone of ResNet18 model\n",
    "\n",
    "import torch.nn as nn  # Neural Network lib\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class MyResNet18(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18 backbone\n",
    "    Inherit PyTorch nn Module, define and train my own ResNet from scratch\n",
    "    num_classes: class num for the model, 91 classes(categories) for COCO dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1000, weights=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int):\n",
    "                Number of output channels from the final linear layer.\n",
    "                For ImageNet classification use 1000; for a custom task like COCO, 91 classes(categories)\n",
    "            weights (bool):\n",
    "                If True, loads ImageNet‑pretrained weights.\n",
    "                If False, train from scratch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # load ResNet18 architecture\n",
    "        self.model = torchvision.models.resnet18(\n",
    "            weights=weights)  # Main feature extractor, no pretrained weights\n",
    "        # replace the final fully connected layer with new classes\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, 3, H, W],\n",
    "                              where B = batch size.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [B, num_classes].\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "3dd4ad25-789f-458d-ba0f-b3a46670aa03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:13:26.175228Z",
     "start_time": "2025-05-24T19:13:26.172525Z"
    }
   },
   "source": [
    "# Transform, image preprocessing\n",
    "from torchvision import transforms\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), # augmentation\n",
    "    transforms.RandomHorizontalFlip(0.5),   # probability of flipping current image\n",
    "    transforms.ToTensor(),              # PIL -> PyTorch FloatTensor\n",
    "    transforms.Normalize(               # data from ImageNet, pixel normalization\n",
    "        mean=[0.5071, 0.4867, 0.4408],   # official CIFAR100 stats\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    )\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:46:36.805065Z",
     "start_time": "2025-05-24T20:46:36.059722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "project_root = Path.cwd().parent            # ➜ NeuralRipper/\n",
    "data_root    = project_root / \"data\"     # folder that already contains “cifar-100-python/”\n",
    "\n",
    "train_ds = CIFAR100(root=str(data_root),\n",
    "                    train=True,\n",
    "                    download=True,          # first run only\n",
    "                    transform=train_tf)\n",
    "\n",
    "val_ds   = CIFAR100(root=str(data_root),\n",
    "                    train=False,\n",
    "                    download=False,\n",
    "                    transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=100, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train / Val sizes ➜ {len(train_ds)} / {len(val_ds)}\")\n"
   ],
   "id": "526c13ce22b06e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Val sizes ➜ 50000 / 10000\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "4acaf169-b005-485a-85ac-6ce026027152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:46:37.436891Z",
     "start_time": "2025-05-24T20:46:37.422935Z"
    }
   },
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Training related functions definitions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_dataset_paths():\n",
    "    \"\"\"\n",
    "    Get appropriate data paths based on environment variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image_directory_path, annotation_file_path)\n",
    "    \"\"\"\n",
    "    use_full = os.environ.get(\"USE_FULL_DATASET\", \"0\").lower() in (\"1\", \"true\", \"yes\")\n",
    "    base_dir = os.path.abspath(\".\")\n",
    "\n",
    "    if use_full:\n",
    "        img_dir = os.path.join(base_dir, \"data\", \"coco\", \"images\", \"train2017\")\n",
    "        ann_file = os.path.join(base_dir, \"data\", \"coco\", \"annotations\", \"instances_train2017.json\")\n",
    "    else:\n",
    "        img_dir = os.path.join(base_dir, \"data\", \"coco\", \"subset\", \"images\")\n",
    "        ann_file = os.path.join(base_dir, \"data\", \"coco\", \"subset\", \"annotations\", \"instances_subset.json\")\n",
    "\n",
    "    return img_dir, ann_file\n",
    "\n",
    "\n",
    "def select_device():\n",
    "    \"\"\"\n",
    "    Select the appropriate device for training.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The device to use for training\n",
    "    \"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def setup_mlflow(batch_size, learning_rate, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Set up and configure the MLFlow for experiment tracking.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Training batch size\n",
    "        learning_rate (float): Learning rate\n",
    "        num_epochs (int): Number of training epochs\n",
    "        device (torch.device): Training device\n",
    "\n",
    "    Returns:\n",
    "        MLFlow instance\n",
    "    \"\"\"\n",
    "\n",
    "    '''\n",
    "    Actual Host where mlflow server deployed\n",
    "    Start mlflow server using\n",
    "    mlflow server \\\n",
    "      --backend-store-uri mysql+pymysql://user:pass@localhost:3306/mlflow \\\n",
    "      --default-artifact-root gs://gcs-bucket/mlflow-artifacts \\\n",
    "      --host 0.0.0.0 --port 5000\n",
    "    '''\n",
    "    MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    print(f\"Tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    # the name of the experiment will show up in mlflow, usually one model one experiment\n",
    "    mlflow.set_experiment(\"ResNet18-CIFAR100\")\n",
    "    mlflow.start_run(run_name=datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"model\": \"ResNet18\",\n",
    "        \"device\": device.type\n",
    "    })\n",
    "\n",
    "\n",
    "def track_metrics(metrics_dict, epoch, step=None, context=None):\n",
    "    \"\"\"\n",
    "    Track multiple metrics in mlflow.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict (dict): Dictionary of metrics to track\n",
    "        epoch (int): Current epoch\n",
    "        step (int, optional): Current step within the epoch\n",
    "        context (dict, optional): Additional context for the metrics\n",
    "    \"\"\"\n",
    "    context = context or {\"subset\": \"train\"}\n",
    "\n",
    "    for name, value in metrics_dict.items():\n",
    "        if step is not None:\n",
    "            mlflow.log_metric(name, value, step=step)\n",
    "        else:\n",
    "            mlflow.log_metric(name, value, step=epoch)\n",
    "\n",
    "\n",
    "def calculate_metrics(all_targets, all_predictions):\n",
    "    \"\"\"\n",
    "    all_targets      - list of (C,) one-hot np.float32\n",
    "    all_predictions  - list of (C,) sigmoid scores np.float32\n",
    "    \"\"\"\n",
    "    y_true  = np.vstack(all_targets).astype(np.int8)     # (N, C)\n",
    "    y_score = np.vstack(all_predictions)                 # (N, C) ∈[0,1]\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Average precision (macro over classes that have ≥1 positive)\n",
    "    aps = []\n",
    "    for c in range(y_true.shape[1]):\n",
    "        if y_true[:, c].sum() == 0:      # skip empty class\n",
    "            continue\n",
    "        aps.append(average_precision_score(y_true[:, c], y_score[:, c]))\n",
    "    metrics[\"avg_precision\"] = float(np.mean(aps)) if aps else 0.0\n",
    "\n",
    "    # ROC AUC (macro over valid classes)\n",
    "    aucs = []\n",
    "    for c in range(y_true.shape[1]):\n",
    "        pos = y_true[:, c].sum()\n",
    "        neg = (1 - y_true[:, c]).sum()\n",
    "        if pos == 0 or neg == 0:\n",
    "            continue                     # undefined\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, c], y_score[:, c])\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "    metrics[\"roc_auc\"] = float(np.mean(aucs)) if aucs else 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for i, (images, targets) in enumerate(loader):\n",
    "        images  = images.to(device)\n",
    "        targets = targets.to(device)           # int labels, NOT one-hot\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss   = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += (preds == targets).sum().item()\n",
    "        running_total   += targets.size(0)\n",
    "        running_loss    += loss.item()\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            avg_loss = running_loss / 10\n",
    "            acc_sofar = running_correct / running_total\n",
    "            logger.info(f\"Epoch {epoch+1} | Batch {i+1} | Loss {avg_loss:.4f} | Acc {acc_sofar:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # FIX: Calculate and return both loss and accuracy\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = running_correct / running_total\n",
    "    return epoch_loss, epoch_acc\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "e8dce4b1-2bd6-4986-8327-8b1e86a1a1e2",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-24T20:54:30.130134Z",
     "start_time": "2025-05-24T20:46:38.217377Z"
    }
   },
   "source": [
    "# main train logic\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "def train():\n",
    "    # Hyperparameters\n",
    "    batch_size    = 64\n",
    "    num_epochs    = 10\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    # Setup\n",
    "    device = select_device()\n",
    "    setup_mlflow(batch_size, learning_rate, num_epochs, device)\n",
    "\n",
    "    # Data & model\n",
    "    train_dataset = train_ds\n",
    "    num_classes   = len(train_dataset.classes)         # CIFAR-100 has 100 classes\n",
    "    model = MyResNet18(num_classes=num_classes, weights=None).to(device)\n",
    "\n",
    "    # Loss & optimizer\n",
    "    criterion = nn.CrossEntropyLoss()                  # single-label classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_acc = float(\"-inf\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # 1 train for one epoch → returns (loss, accuracy)\n",
    "        epoch_loss, epoch_acc = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device, epoch\n",
    "        )\n",
    "\n",
    "        # 2 log best model to MLflow\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "\n",
    "            # package & log model for serving\n",
    "            sample_in  = torch.randn(1, 3, 224, 224, dtype=torch.float32).to(device)\n",
    "            signature = infer_signature(\n",
    "                sample_in.cpu().numpy(),\n",
    "                model(sample_in).detach().cpu().numpy()\n",
    "            )\n",
    "            pip_reqs = [\n",
    "                f\"torch=={torch.__version__}\",\n",
    "                f\"torchvision=={torchvision.__version__}\",\n",
    "            ]\n",
    "            mlflow.pytorch.log_model(\n",
    "                model,\n",
    "                artifact_path=\"best_model\",\n",
    "                signature=signature,\n",
    "                pip_requirements=pip_reqs\n",
    "            )\n",
    "\n",
    "        # 3 log epoch‐level metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"train_accuracy\": epoch_acc,\n",
    "            \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "        }, step=epoch)\n",
    "\n",
    "        # 4 console output\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} — \"\n",
    "              f\"Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "# run with exception handling\n",
    "try:\n",
    "    train()\n",
    "except Exception as e:\n",
    "    print(f\"Train failed: {e}\")\n",
    "    mlflow.end_run(status=\"FAILED\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:46:38,223 - INFO - Using device: mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI set to: http://localhost:5000\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:46:45,284 - INFO - Epoch 1 | Batch 10 | Loss 4.8116 | Acc 0.0109\n",
      "2025-05-24 13:46:45,870 - INFO - Epoch 1 | Batch 20 | Loss 4.7233 | Acc 0.0156\n",
      "2025-05-24 13:46:46,361 - INFO - Epoch 1 | Batch 30 | Loss 4.6409 | Acc 0.0180\n",
      "2025-05-24 13:46:46,850 - INFO - Epoch 1 | Batch 40 | Loss 4.4675 | Acc 0.0223\n",
      "2025-05-24 13:46:47,377 - INFO - Epoch 1 | Batch 50 | Loss 4.4395 | Acc 0.0277\n",
      "2025-05-24 13:46:47,864 - INFO - Epoch 1 | Batch 60 | Loss 4.3829 | Acc 0.0319\n",
      "2025-05-24 13:46:48,346 - INFO - Epoch 1 | Batch 70 | Loss 4.3463 | Acc 0.0358\n",
      "2025-05-24 13:46:48,838 - INFO - Epoch 1 | Batch 80 | Loss 4.2734 | Acc 0.0389\n",
      "2025-05-24 13:46:49,320 - INFO - Epoch 1 | Batch 90 | Loss 4.2470 | Acc 0.0411\n",
      "2025-05-24 13:46:49,806 - INFO - Epoch 1 | Batch 100 | Loss 4.2239 | Acc 0.0443\n",
      "2025-05-24 13:46:50,281 - INFO - Epoch 1 | Batch 110 | Loss 4.1824 | Acc 0.0474\n",
      "2025-05-24 13:46:50,771 - INFO - Epoch 1 | Batch 120 | Loss 4.1105 | Acc 0.0493\n",
      "2025-05-24 13:46:51,266 - INFO - Epoch 1 | Batch 130 | Loss 4.1390 | Acc 0.0518\n",
      "2025-05-24 13:46:51,764 - INFO - Epoch 1 | Batch 140 | Loss 4.0113 | Acc 0.0546\n",
      "2025-05-24 13:46:52,246 - INFO - Epoch 1 | Batch 150 | Loss 4.0288 | Acc 0.0571\n",
      "2025-05-24 13:46:52,728 - INFO - Epoch 1 | Batch 160 | Loss 4.0451 | Acc 0.0580\n",
      "2025-05-24 13:46:53,206 - INFO - Epoch 1 | Batch 170 | Loss 4.0219 | Acc 0.0602\n",
      "2025-05-24 13:46:53,689 - INFO - Epoch 1 | Batch 180 | Loss 4.0211 | Acc 0.0618\n",
      "2025-05-24 13:46:54,173 - INFO - Epoch 1 | Batch 190 | Loss 3.9285 | Acc 0.0639\n",
      "2025-05-24 13:46:54,652 - INFO - Epoch 1 | Batch 200 | Loss 3.9285 | Acc 0.0664\n",
      "2025-05-24 13:46:55,131 - INFO - Epoch 1 | Batch 210 | Loss 3.9608 | Acc 0.0682\n",
      "2025-05-24 13:46:55,613 - INFO - Epoch 1 | Batch 220 | Loss 3.8748 | Acc 0.0701\n",
      "2025-05-24 13:46:56,092 - INFO - Epoch 1 | Batch 230 | Loss 3.8956 | Acc 0.0723\n",
      "2025-05-24 13:46:56,568 - INFO - Epoch 1 | Batch 240 | Loss 3.8512 | Acc 0.0744\n",
      "2025-05-24 13:46:57,047 - INFO - Epoch 1 | Batch 250 | Loss 3.8607 | Acc 0.0760\n",
      "2025-05-24 13:46:57,528 - INFO - Epoch 1 | Batch 260 | Loss 3.8699 | Acc 0.0775\n",
      "2025-05-24 13:46:58,010 - INFO - Epoch 1 | Batch 270 | Loss 3.7568 | Acc 0.0796\n",
      "2025-05-24 13:46:58,488 - INFO - Epoch 1 | Batch 280 | Loss 3.8193 | Acc 0.0805\n",
      "2025-05-24 13:46:58,971 - INFO - Epoch 1 | Batch 290 | Loss 3.8179 | Acc 0.0817\n",
      "2025-05-24 13:46:59,452 - INFO - Epoch 1 | Batch 300 | Loss 3.8089 | Acc 0.0838\n",
      "2025-05-24 13:46:59,933 - INFO - Epoch 1 | Batch 310 | Loss 3.7897 | Acc 0.0851\n",
      "2025-05-24 13:47:00,414 - INFO - Epoch 1 | Batch 320 | Loss 3.7233 | Acc 0.0863\n",
      "2025-05-24 13:47:00,898 - INFO - Epoch 1 | Batch 330 | Loss 3.7217 | Acc 0.0875\n",
      "2025-05-24 13:47:01,376 - INFO - Epoch 1 | Batch 340 | Loss 3.7695 | Acc 0.0888\n",
      "2025-05-24 13:47:01,855 - INFO - Epoch 1 | Batch 350 | Loss 3.7387 | Acc 0.0899\n",
      "2025-05-24 13:47:02,331 - INFO - Epoch 1 | Batch 360 | Loss 3.6810 | Acc 0.0915\n",
      "2025-05-24 13:47:02,812 - INFO - Epoch 1 | Batch 370 | Loss 3.7014 | Acc 0.0928\n",
      "2025-05-24 13:47:03,294 - INFO - Epoch 1 | Batch 380 | Loss 3.6149 | Acc 0.0941\n",
      "2025-05-24 13:47:03,780 - INFO - Epoch 1 | Batch 390 | Loss 3.7205 | Acc 0.0949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 1/10 — Loss: 0.0095  Acc: 0.0950\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yj/Workspaces/NeuralRipper/backend/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-05-24 13:47:32,968 - INFO - Epoch 2 | Batch 10 | Loss 3.6308 | Acc 0.1555\n",
      "2025-05-24 13:47:33,458 - INFO - Epoch 2 | Batch 20 | Loss 3.6401 | Acc 0.1516\n",
      "2025-05-24 13:47:33,944 - INFO - Epoch 2 | Batch 30 | Loss 3.6072 | Acc 0.1529\n",
      "2025-05-24 13:47:34,429 - INFO - Epoch 2 | Batch 40 | Loss 3.5500 | Acc 0.1545\n",
      "2025-05-24 13:47:34,914 - INFO - Epoch 2 | Batch 50 | Loss 3.5752 | Acc 0.1573\n",
      "2025-05-24 13:47:35,400 - INFO - Epoch 2 | Batch 60 | Loss 3.5311 | Acc 0.1612\n",
      "2025-05-24 13:47:35,887 - INFO - Epoch 2 | Batch 70 | Loss 3.5670 | Acc 0.1622\n",
      "2025-05-24 13:47:36,363 - INFO - Epoch 2 | Batch 80 | Loss 3.5773 | Acc 0.1635\n",
      "2025-05-24 13:47:36,838 - INFO - Epoch 2 | Batch 90 | Loss 3.5382 | Acc 0.1626\n",
      "2025-05-24 13:47:37,315 - INFO - Epoch 2 | Batch 100 | Loss 3.5699 | Acc 0.1622\n",
      "2025-05-24 13:47:37,795 - INFO - Epoch 2 | Batch 110 | Loss 3.5619 | Acc 0.1615\n",
      "2025-05-24 13:47:38,277 - INFO - Epoch 2 | Batch 120 | Loss 3.5472 | Acc 0.1608\n",
      "2025-05-24 13:47:38,751 - INFO - Epoch 2 | Batch 130 | Loss 3.5625 | Acc 0.1615\n",
      "2025-05-24 13:47:39,227 - INFO - Epoch 2 | Batch 140 | Loss 3.5320 | Acc 0.1614\n",
      "2025-05-24 13:47:39,702 - INFO - Epoch 2 | Batch 150 | Loss 3.4881 | Acc 0.1614\n",
      "2025-05-24 13:47:40,178 - INFO - Epoch 2 | Batch 160 | Loss 3.5006 | Acc 0.1620\n",
      "2025-05-24 13:47:40,658 - INFO - Epoch 2 | Batch 170 | Loss 3.5100 | Acc 0.1622\n",
      "2025-05-24 13:47:41,135 - INFO - Epoch 2 | Batch 180 | Loss 3.4963 | Acc 0.1622\n",
      "2025-05-24 13:47:41,614 - INFO - Epoch 2 | Batch 190 | Loss 3.5287 | Acc 0.1619\n",
      "2025-05-24 13:47:42,089 - INFO - Epoch 2 | Batch 200 | Loss 3.4714 | Acc 0.1626\n",
      "2025-05-24 13:47:42,563 - INFO - Epoch 2 | Batch 210 | Loss 3.4029 | Acc 0.1639\n",
      "2025-05-24 13:47:43,037 - INFO - Epoch 2 | Batch 220 | Loss 3.4445 | Acc 0.1648\n",
      "2025-05-24 13:47:43,514 - INFO - Epoch 2 | Batch 230 | Loss 3.4873 | Acc 0.1650\n",
      "2025-05-24 13:47:43,990 - INFO - Epoch 2 | Batch 240 | Loss 3.4505 | Acc 0.1660\n",
      "2025-05-24 13:47:44,468 - INFO - Epoch 2 | Batch 250 | Loss 3.4421 | Acc 0.1661\n",
      "2025-05-24 13:47:44,944 - INFO - Epoch 2 | Batch 260 | Loss 3.4029 | Acc 0.1662\n",
      "2025-05-24 13:47:45,422 - INFO - Epoch 2 | Batch 270 | Loss 3.4115 | Acc 0.1674\n",
      "2025-05-24 13:47:45,900 - INFO - Epoch 2 | Batch 280 | Loss 3.4126 | Acc 0.1682\n",
      "2025-05-24 13:47:46,378 - INFO - Epoch 2 | Batch 290 | Loss 3.3777 | Acc 0.1683\n",
      "2025-05-24 13:47:46,856 - INFO - Epoch 2 | Batch 300 | Loss 3.4398 | Acc 0.1690\n",
      "2025-05-24 13:47:47,331 - INFO - Epoch 2 | Batch 310 | Loss 3.4043 | Acc 0.1692\n",
      "2025-05-24 13:47:47,809 - INFO - Epoch 2 | Batch 320 | Loss 3.3939 | Acc 0.1692\n",
      "2025-05-24 13:47:48,285 - INFO - Epoch 2 | Batch 330 | Loss 3.4413 | Acc 0.1698\n",
      "2025-05-24 13:47:48,759 - INFO - Epoch 2 | Batch 340 | Loss 3.4105 | Acc 0.1704\n",
      "2025-05-24 13:47:49,235 - INFO - Epoch 2 | Batch 350 | Loss 3.4285 | Acc 0.1704\n",
      "2025-05-24 13:47:49,716 - INFO - Epoch 2 | Batch 360 | Loss 3.3604 | Acc 0.1710\n",
      "2025-05-24 13:47:50,194 - INFO - Epoch 2 | Batch 370 | Loss 3.3304 | Acc 0.1718\n",
      "2025-05-24 13:47:50,675 - INFO - Epoch 2 | Batch 380 | Loss 3.3859 | Acc 0.1721\n",
      "2025-05-24 13:47:51,157 - INFO - Epoch 2 | Batch 390 | Loss 3.3583 | Acc 0.1726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 2/10 — Loss: 0.0081  Acc: 0.1727\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:48:19,591 - INFO - Epoch 3 | Batch 10 | Loss 3.2889 | Acc 0.2094\n",
      "2025-05-24 13:48:20,071 - INFO - Epoch 3 | Batch 20 | Loss 3.2828 | Acc 0.2066\n",
      "2025-05-24 13:48:20,543 - INFO - Epoch 3 | Batch 30 | Loss 3.2937 | Acc 0.2065\n",
      "2025-05-24 13:48:21,016 - INFO - Epoch 3 | Batch 40 | Loss 3.3794 | Acc 0.2002\n",
      "2025-05-24 13:48:21,487 - INFO - Epoch 3 | Batch 50 | Loss 3.2745 | Acc 0.2003\n",
      "2025-05-24 13:48:21,960 - INFO - Epoch 3 | Batch 60 | Loss 3.2656 | Acc 0.2000\n",
      "2025-05-24 13:48:22,437 - INFO - Epoch 3 | Batch 70 | Loss 3.2412 | Acc 0.2012\n",
      "2025-05-24 13:48:22,916 - INFO - Epoch 3 | Batch 80 | Loss 3.2238 | Acc 0.2030\n",
      "2025-05-24 13:48:23,394 - INFO - Epoch 3 | Batch 90 | Loss 3.3182 | Acc 0.2027\n",
      "2025-05-24 13:48:23,871 - INFO - Epoch 3 | Batch 100 | Loss 3.2979 | Acc 0.2030\n",
      "2025-05-24 13:48:24,350 - INFO - Epoch 3 | Batch 110 | Loss 3.2632 | Acc 0.2033\n",
      "2025-05-24 13:48:24,827 - INFO - Epoch 3 | Batch 120 | Loss 3.2037 | Acc 0.2058\n",
      "2025-05-24 13:48:25,304 - INFO - Epoch 3 | Batch 130 | Loss 3.2312 | Acc 0.2067\n",
      "2025-05-24 13:48:25,782 - INFO - Epoch 3 | Batch 140 | Loss 3.2221 | Acc 0.2079\n",
      "2025-05-24 13:48:26,257 - INFO - Epoch 3 | Batch 150 | Loss 3.2032 | Acc 0.2091\n",
      "2025-05-24 13:48:26,730 - INFO - Epoch 3 | Batch 160 | Loss 3.2052 | Acc 0.2102\n",
      "2025-05-24 13:48:27,204 - INFO - Epoch 3 | Batch 170 | Loss 3.2739 | Acc 0.2085\n",
      "2025-05-24 13:48:27,683 - INFO - Epoch 3 | Batch 180 | Loss 3.2161 | Acc 0.2099\n",
      "2025-05-24 13:48:28,161 - INFO - Epoch 3 | Batch 190 | Loss 3.3081 | Acc 0.2098\n",
      "2025-05-24 13:48:28,637 - INFO - Epoch 3 | Batch 200 | Loss 3.2107 | Acc 0.2109\n",
      "2025-05-24 13:48:29,119 - INFO - Epoch 3 | Batch 210 | Loss 3.2194 | Acc 0.2117\n",
      "2025-05-24 13:48:29,599 - INFO - Epoch 3 | Batch 220 | Loss 3.1769 | Acc 0.2120\n",
      "2025-05-24 13:48:30,078 - INFO - Epoch 3 | Batch 230 | Loss 3.1936 | Acc 0.2118\n",
      "2025-05-24 13:48:30,557 - INFO - Epoch 3 | Batch 240 | Loss 3.1787 | Acc 0.2123\n",
      "2025-05-24 13:48:31,035 - INFO - Epoch 3 | Batch 250 | Loss 3.1351 | Acc 0.2131\n",
      "2025-05-24 13:48:31,513 - INFO - Epoch 3 | Batch 260 | Loss 3.2212 | Acc 0.2137\n",
      "2025-05-24 13:48:31,990 - INFO - Epoch 3 | Batch 270 | Loss 3.1872 | Acc 0.2141\n",
      "2025-05-24 13:48:32,464 - INFO - Epoch 3 | Batch 280 | Loss 3.2211 | Acc 0.2139\n",
      "2025-05-24 13:48:32,940 - INFO - Epoch 3 | Batch 290 | Loss 3.2030 | Acc 0.2140\n",
      "2025-05-24 13:48:33,415 - INFO - Epoch 3 | Batch 300 | Loss 3.1931 | Acc 0.2141\n",
      "2025-05-24 13:48:33,889 - INFO - Epoch 3 | Batch 310 | Loss 3.1675 | Acc 0.2143\n",
      "2025-05-24 13:48:34,371 - INFO - Epoch 3 | Batch 320 | Loss 3.2278 | Acc 0.2145\n",
      "2025-05-24 13:48:34,850 - INFO - Epoch 3 | Batch 330 | Loss 3.1703 | Acc 0.2147\n",
      "2025-05-24 13:48:35,335 - INFO - Epoch 3 | Batch 340 | Loss 3.1444 | Acc 0.2155\n",
      "2025-05-24 13:48:35,816 - INFO - Epoch 3 | Batch 350 | Loss 3.1210 | Acc 0.2156\n",
      "2025-05-24 13:48:36,298 - INFO - Epoch 3 | Batch 360 | Loss 3.1262 | Acc 0.2162\n",
      "2025-05-24 13:48:36,777 - INFO - Epoch 3 | Batch 370 | Loss 3.1282 | Acc 0.2167\n",
      "2025-05-24 13:48:37,257 - INFO - Epoch 3 | Batch 380 | Loss 3.1772 | Acc 0.2166\n",
      "2025-05-24 13:48:37,736 - INFO - Epoch 3 | Batch 390 | Loss 3.1224 | Acc 0.2173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 3/10 — Loss: 0.0082  Acc: 0.2173\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:49:06,059 - INFO - Epoch 4 | Batch 10 | Loss 3.0908 | Acc 0.2352\n",
      "2025-05-24 13:49:06,542 - INFO - Epoch 4 | Batch 20 | Loss 2.9740 | Acc 0.2461\n",
      "2025-05-24 13:49:07,011 - INFO - Epoch 4 | Batch 30 | Loss 3.0610 | Acc 0.2466\n",
      "2025-05-24 13:49:07,484 - INFO - Epoch 4 | Batch 40 | Loss 2.9857 | Acc 0.2527\n",
      "2025-05-24 13:49:07,959 - INFO - Epoch 4 | Batch 50 | Loss 3.0397 | Acc 0.2502\n",
      "2025-05-24 13:49:08,435 - INFO - Epoch 4 | Batch 60 | Loss 3.1225 | Acc 0.2452\n",
      "2025-05-24 13:49:08,912 - INFO - Epoch 4 | Batch 70 | Loss 3.1401 | Acc 0.2426\n",
      "2025-05-24 13:49:09,391 - INFO - Epoch 4 | Batch 80 | Loss 3.0920 | Acc 0.2414\n",
      "2025-05-24 13:49:09,867 - INFO - Epoch 4 | Batch 90 | Loss 3.1124 | Acc 0.2395\n",
      "2025-05-24 13:49:10,345 - INFO - Epoch 4 | Batch 100 | Loss 3.0549 | Acc 0.2393\n",
      "2025-05-24 13:49:10,824 - INFO - Epoch 4 | Batch 110 | Loss 3.0552 | Acc 0.2405\n",
      "2025-05-24 13:49:11,300 - INFO - Epoch 4 | Batch 120 | Loss 3.0442 | Acc 0.2419\n",
      "2025-05-24 13:49:11,781 - INFO - Epoch 4 | Batch 130 | Loss 3.0669 | Acc 0.2425\n",
      "2025-05-24 13:49:12,254 - INFO - Epoch 4 | Batch 140 | Loss 2.9850 | Acc 0.2435\n",
      "2025-05-24 13:49:12,730 - INFO - Epoch 4 | Batch 150 | Loss 3.0122 | Acc 0.2434\n",
      "2025-05-24 13:49:13,203 - INFO - Epoch 4 | Batch 160 | Loss 3.0645 | Acc 0.2437\n",
      "2025-05-24 13:49:13,678 - INFO - Epoch 4 | Batch 170 | Loss 3.0435 | Acc 0.2437\n",
      "2025-05-24 13:49:14,157 - INFO - Epoch 4 | Batch 180 | Loss 3.0461 | Acc 0.2445\n",
      "2025-05-24 13:49:14,636 - INFO - Epoch 4 | Batch 190 | Loss 3.0080 | Acc 0.2456\n",
      "2025-05-24 13:49:15,118 - INFO - Epoch 4 | Batch 200 | Loss 3.0914 | Acc 0.2448\n",
      "2025-05-24 13:49:15,599 - INFO - Epoch 4 | Batch 210 | Loss 3.0497 | Acc 0.2444\n",
      "2025-05-24 13:49:16,078 - INFO - Epoch 4 | Batch 220 | Loss 2.9383 | Acc 0.2456\n",
      "2025-05-24 13:49:16,558 - INFO - Epoch 4 | Batch 230 | Loss 3.0646 | Acc 0.2454\n",
      "2025-05-24 13:49:17,035 - INFO - Epoch 4 | Batch 240 | Loss 3.0416 | Acc 0.2453\n",
      "2025-05-24 13:49:17,512 - INFO - Epoch 4 | Batch 250 | Loss 3.0008 | Acc 0.2458\n",
      "2025-05-24 13:49:17,986 - INFO - Epoch 4 | Batch 260 | Loss 2.9787 | Acc 0.2466\n",
      "2025-05-24 13:49:18,464 - INFO - Epoch 4 | Batch 270 | Loss 3.0615 | Acc 0.2457\n",
      "2025-05-24 13:49:18,942 - INFO - Epoch 4 | Batch 280 | Loss 3.0080 | Acc 0.2459\n",
      "2025-05-24 13:49:19,419 - INFO - Epoch 4 | Batch 290 | Loss 3.0519 | Acc 0.2461\n",
      "2025-05-24 13:49:19,901 - INFO - Epoch 4 | Batch 300 | Loss 3.0627 | Acc 0.2457\n",
      "2025-05-24 13:49:20,381 - INFO - Epoch 4 | Batch 310 | Loss 3.0415 | Acc 0.2457\n",
      "2025-05-24 13:49:20,863 - INFO - Epoch 4 | Batch 320 | Loss 2.9787 | Acc 0.2464\n",
      "2025-05-24 13:49:21,350 - INFO - Epoch 4 | Batch 330 | Loss 2.9472 | Acc 0.2470\n",
      "2025-05-24 13:49:21,836 - INFO - Epoch 4 | Batch 340 | Loss 2.9998 | Acc 0.2474\n",
      "2025-05-24 13:49:22,365 - INFO - Epoch 4 | Batch 350 | Loss 2.9937 | Acc 0.2477\n",
      "2025-05-24 13:49:22,847 - INFO - Epoch 4 | Batch 360 | Loss 3.0011 | Acc 0.2479\n",
      "2025-05-24 13:49:23,337 - INFO - Epoch 4 | Batch 370 | Loss 2.8602 | Acc 0.2491\n",
      "2025-05-24 13:49:23,824 - INFO - Epoch 4 | Batch 380 | Loss 2.9881 | Acc 0.2496\n",
      "2025-05-24 13:49:24,308 - INFO - Epoch 4 | Batch 390 | Loss 3.0202 | Acc 0.2498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 4/10 — Loss: 0.0068  Acc: 0.2499\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:49:52,815 - INFO - Epoch 5 | Batch 10 | Loss 2.9754 | Acc 0.2648\n",
      "2025-05-24 13:49:53,301 - INFO - Epoch 5 | Batch 20 | Loss 2.9545 | Acc 0.2684\n",
      "2025-05-24 13:49:53,775 - INFO - Epoch 5 | Batch 30 | Loss 2.9117 | Acc 0.2693\n",
      "2025-05-24 13:49:54,246 - INFO - Epoch 5 | Batch 40 | Loss 2.8466 | Acc 0.2762\n",
      "2025-05-24 13:49:54,720 - INFO - Epoch 5 | Batch 50 | Loss 2.9047 | Acc 0.2762\n",
      "2025-05-24 13:49:55,202 - INFO - Epoch 5 | Batch 60 | Loss 2.7922 | Acc 0.2814\n",
      "2025-05-24 13:49:55,685 - INFO - Epoch 5 | Batch 70 | Loss 2.8505 | Acc 0.2811\n",
      "2025-05-24 13:49:56,165 - INFO - Epoch 5 | Batch 80 | Loss 2.8975 | Acc 0.2796\n",
      "2025-05-24 13:49:56,640 - INFO - Epoch 5 | Batch 90 | Loss 2.8913 | Acc 0.2779\n",
      "2025-05-24 13:49:57,120 - INFO - Epoch 5 | Batch 100 | Loss 2.9993 | Acc 0.2745\n",
      "2025-05-24 13:49:57,613 - INFO - Epoch 5 | Batch 110 | Loss 2.9452 | Acc 0.2732\n",
      "2025-05-24 13:49:58,108 - INFO - Epoch 5 | Batch 120 | Loss 2.9022 | Acc 0.2732\n",
      "2025-05-24 13:49:58,594 - INFO - Epoch 5 | Batch 130 | Loss 2.8739 | Acc 0.2739\n",
      "2025-05-24 13:49:59,077 - INFO - Epoch 5 | Batch 140 | Loss 2.9119 | Acc 0.2735\n",
      "2025-05-24 13:49:59,562 - INFO - Epoch 5 | Batch 150 | Loss 2.8652 | Acc 0.2742\n",
      "2025-05-24 13:50:00,042 - INFO - Epoch 5 | Batch 160 | Loss 2.9233 | Acc 0.2734\n",
      "2025-05-24 13:50:00,523 - INFO - Epoch 5 | Batch 170 | Loss 2.8264 | Acc 0.2756\n",
      "2025-05-24 13:50:01,005 - INFO - Epoch 5 | Batch 180 | Loss 2.8195 | Acc 0.2754\n",
      "2025-05-24 13:50:01,488 - INFO - Epoch 5 | Batch 190 | Loss 2.9056 | Acc 0.2753\n",
      "2025-05-24 13:50:01,968 - INFO - Epoch 5 | Batch 200 | Loss 2.8766 | Acc 0.2756\n",
      "2025-05-24 13:50:02,451 - INFO - Epoch 5 | Batch 210 | Loss 2.8759 | Acc 0.2761\n",
      "2025-05-24 13:50:02,932 - INFO - Epoch 5 | Batch 220 | Loss 2.8726 | Acc 0.2768\n",
      "2025-05-24 13:50:03,418 - INFO - Epoch 5 | Batch 230 | Loss 2.8562 | Acc 0.2768\n",
      "2025-05-24 13:50:03,899 - INFO - Epoch 5 | Batch 240 | Loss 2.8739 | Acc 0.2767\n",
      "2025-05-24 13:50:04,384 - INFO - Epoch 5 | Batch 250 | Loss 2.8663 | Acc 0.2767\n",
      "2025-05-24 13:50:04,864 - INFO - Epoch 5 | Batch 260 | Loss 2.9229 | Acc 0.2762\n",
      "2025-05-24 13:50:05,347 - INFO - Epoch 5 | Batch 270 | Loss 2.8139 | Acc 0.2777\n",
      "2025-05-24 13:50:05,829 - INFO - Epoch 5 | Batch 280 | Loss 2.8634 | Acc 0.2778\n",
      "2025-05-24 13:50:06,314 - INFO - Epoch 5 | Batch 290 | Loss 2.8678 | Acc 0.2777\n",
      "2025-05-24 13:50:06,796 - INFO - Epoch 5 | Batch 300 | Loss 2.8476 | Acc 0.2780\n",
      "2025-05-24 13:50:07,283 - INFO - Epoch 5 | Batch 310 | Loss 2.8137 | Acc 0.2787\n",
      "2025-05-24 13:50:07,765 - INFO - Epoch 5 | Batch 320 | Loss 2.8679 | Acc 0.2792\n",
      "2025-05-24 13:50:08,246 - INFO - Epoch 5 | Batch 330 | Loss 2.9083 | Acc 0.2791\n",
      "2025-05-24 13:50:08,727 - INFO - Epoch 5 | Batch 340 | Loss 2.8360 | Acc 0.2796\n",
      "2025-05-24 13:50:09,212 - INFO - Epoch 5 | Batch 350 | Loss 2.9278 | Acc 0.2792\n",
      "2025-05-24 13:50:09,693 - INFO - Epoch 5 | Batch 360 | Loss 2.8457 | Acc 0.2792\n",
      "2025-05-24 13:50:10,174 - INFO - Epoch 5 | Batch 370 | Loss 2.9156 | Acc 0.2792\n",
      "2025-05-24 13:50:10,657 - INFO - Epoch 5 | Batch 380 | Loss 2.7928 | Acc 0.2795\n",
      "2025-05-24 13:50:11,140 - INFO - Epoch 5 | Batch 390 | Loss 2.8342 | Acc 0.2797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 5/10 — Loss: 0.0071  Acc: 0.2797\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:50:40,184 - INFO - Epoch 6 | Batch 10 | Loss 2.7601 | Acc 0.3125\n",
      "2025-05-24 13:50:40,696 - INFO - Epoch 6 | Batch 20 | Loss 2.8107 | Acc 0.2969\n",
      "2025-05-24 13:50:41,206 - INFO - Epoch 6 | Batch 30 | Loss 2.7673 | Acc 0.2979\n",
      "2025-05-24 13:50:41,688 - INFO - Epoch 6 | Batch 40 | Loss 2.7648 | Acc 0.2963\n",
      "2025-05-24 13:50:42,177 - INFO - Epoch 6 | Batch 50 | Loss 2.7630 | Acc 0.2998\n",
      "2025-05-24 13:50:42,673 - INFO - Epoch 6 | Batch 60 | Loss 2.7531 | Acc 0.2987\n",
      "2025-05-24 13:50:43,168 - INFO - Epoch 6 | Batch 70 | Loss 2.8220 | Acc 0.2971\n",
      "2025-05-24 13:50:43,645 - INFO - Epoch 6 | Batch 80 | Loss 2.7914 | Acc 0.2943\n",
      "2025-05-24 13:50:44,121 - INFO - Epoch 6 | Batch 90 | Loss 2.7947 | Acc 0.2951\n",
      "2025-05-24 13:50:44,600 - INFO - Epoch 6 | Batch 100 | Loss 2.8198 | Acc 0.2952\n",
      "2025-05-24 13:50:45,078 - INFO - Epoch 6 | Batch 110 | Loss 2.7166 | Acc 0.2963\n",
      "2025-05-24 13:50:45,560 - INFO - Epoch 6 | Batch 120 | Loss 2.7486 | Acc 0.2957\n",
      "2025-05-24 13:50:46,047 - INFO - Epoch 6 | Batch 130 | Loss 2.7075 | Acc 0.2977\n",
      "2025-05-24 13:50:46,533 - INFO - Epoch 6 | Batch 140 | Loss 2.6792 | Acc 0.2983\n",
      "2025-05-24 13:50:47,012 - INFO - Epoch 6 | Batch 150 | Loss 2.7288 | Acc 0.2989\n",
      "2025-05-24 13:50:47,566 - INFO - Epoch 6 | Batch 160 | Loss 2.7944 | Acc 0.2979\n",
      "2025-05-24 13:50:48,066 - INFO - Epoch 6 | Batch 170 | Loss 2.7710 | Acc 0.2980\n",
      "2025-05-24 13:50:48,603 - INFO - Epoch 6 | Batch 180 | Loss 2.7466 | Acc 0.2983\n",
      "2025-05-24 13:50:49,086 - INFO - Epoch 6 | Batch 190 | Loss 2.7305 | Acc 0.2986\n",
      "2025-05-24 13:50:49,567 - INFO - Epoch 6 | Batch 200 | Loss 2.7592 | Acc 0.2989\n",
      "2025-05-24 13:50:50,045 - INFO - Epoch 6 | Batch 210 | Loss 2.7535 | Acc 0.2990\n",
      "2025-05-24 13:50:50,528 - INFO - Epoch 6 | Batch 220 | Loss 2.7104 | Acc 0.2994\n",
      "2025-05-24 13:50:51,007 - INFO - Epoch 6 | Batch 230 | Loss 2.7529 | Acc 0.2998\n",
      "2025-05-24 13:50:51,485 - INFO - Epoch 6 | Batch 240 | Loss 2.6327 | Acc 0.3006\n",
      "2025-05-24 13:50:51,967 - INFO - Epoch 6 | Batch 250 | Loss 2.7340 | Acc 0.3006\n",
      "2025-05-24 13:50:52,455 - INFO - Epoch 6 | Batch 260 | Loss 2.7655 | Acc 0.3005\n",
      "2025-05-24 13:50:52,939 - INFO - Epoch 6 | Batch 270 | Loss 2.7862 | Acc 0.3002\n",
      "2025-05-24 13:50:53,427 - INFO - Epoch 6 | Batch 280 | Loss 2.7652 | Acc 0.2999\n",
      "2025-05-24 13:50:53,910 - INFO - Epoch 6 | Batch 290 | Loss 2.6861 | Acc 0.3003\n",
      "2025-05-24 13:50:54,391 - INFO - Epoch 6 | Batch 300 | Loss 2.7313 | Acc 0.3005\n",
      "2025-05-24 13:50:54,898 - INFO - Epoch 6 | Batch 310 | Loss 2.6908 | Acc 0.3011\n",
      "2025-05-24 13:50:55,395 - INFO - Epoch 6 | Batch 320 | Loss 2.6705 | Acc 0.3018\n",
      "2025-05-24 13:50:55,873 - INFO - Epoch 6 | Batch 330 | Loss 2.7453 | Acc 0.3019\n",
      "2025-05-24 13:50:56,353 - INFO - Epoch 6 | Batch 340 | Loss 2.7136 | Acc 0.3022\n",
      "2025-05-24 13:50:56,837 - INFO - Epoch 6 | Batch 350 | Loss 2.7408 | Acc 0.3018\n",
      "2025-05-24 13:50:57,312 - INFO - Epoch 6 | Batch 360 | Loss 2.6639 | Acc 0.3024\n",
      "2025-05-24 13:50:57,802 - INFO - Epoch 6 | Batch 370 | Loss 2.7706 | Acc 0.3027\n",
      "2025-05-24 13:50:58,281 - INFO - Epoch 6 | Batch 380 | Loss 2.7679 | Acc 0.3027\n",
      "2025-05-24 13:50:58,769 - INFO - Epoch 6 | Batch 390 | Loss 2.7239 | Acc 0.3030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 6/10 — Loss: 0.0067  Acc: 0.3030\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:51:27,313 - INFO - Epoch 7 | Batch 10 | Loss 2.6563 | Acc 0.3234\n",
      "2025-05-24 13:51:27,805 - INFO - Epoch 7 | Batch 20 | Loss 2.6811 | Acc 0.3230\n",
      "2025-05-24 13:51:28,316 - INFO - Epoch 7 | Batch 30 | Loss 2.6091 | Acc 0.3260\n",
      "2025-05-24 13:51:28,849 - INFO - Epoch 7 | Batch 40 | Loss 2.6560 | Acc 0.3256\n",
      "2025-05-24 13:51:29,349 - INFO - Epoch 7 | Batch 50 | Loss 2.6672 | Acc 0.3262\n",
      "2025-05-24 13:51:29,851 - INFO - Epoch 7 | Batch 60 | Loss 2.6502 | Acc 0.3264\n",
      "2025-05-24 13:51:30,332 - INFO - Epoch 7 | Batch 70 | Loss 2.6236 | Acc 0.3251\n",
      "2025-05-24 13:51:30,837 - INFO - Epoch 7 | Batch 80 | Loss 2.6539 | Acc 0.3266\n",
      "2025-05-24 13:51:31,369 - INFO - Epoch 7 | Batch 90 | Loss 2.6754 | Acc 0.3244\n",
      "2025-05-24 13:51:31,877 - INFO - Epoch 7 | Batch 100 | Loss 2.6795 | Acc 0.3234\n",
      "2025-05-24 13:51:32,358 - INFO - Epoch 7 | Batch 110 | Loss 2.5610 | Acc 0.3249\n",
      "2025-05-24 13:51:32,852 - INFO - Epoch 7 | Batch 120 | Loss 2.6546 | Acc 0.3250\n",
      "2025-05-24 13:51:33,377 - INFO - Epoch 7 | Batch 130 | Loss 2.5738 | Acc 0.3269\n",
      "2025-05-24 13:51:33,870 - INFO - Epoch 7 | Batch 140 | Loss 2.6285 | Acc 0.3273\n",
      "2025-05-24 13:51:34,394 - INFO - Epoch 7 | Batch 150 | Loss 2.6318 | Acc 0.3279\n",
      "2025-05-24 13:51:34,873 - INFO - Epoch 7 | Batch 160 | Loss 2.5907 | Acc 0.3284\n",
      "2025-05-24 13:51:35,362 - INFO - Epoch 7 | Batch 170 | Loss 2.5471 | Acc 0.3294\n",
      "2025-05-24 13:51:35,863 - INFO - Epoch 7 | Batch 180 | Loss 2.7299 | Acc 0.3284\n",
      "2025-05-24 13:51:36,366 - INFO - Epoch 7 | Batch 190 | Loss 2.7078 | Acc 0.3281\n",
      "2025-05-24 13:51:36,858 - INFO - Epoch 7 | Batch 200 | Loss 2.7774 | Acc 0.3263\n",
      "2025-05-24 13:51:37,343 - INFO - Epoch 7 | Batch 210 | Loss 2.6018 | Acc 0.3263\n",
      "2025-05-24 13:51:37,827 - INFO - Epoch 7 | Batch 220 | Loss 2.5281 | Acc 0.3278\n",
      "2025-05-24 13:51:38,358 - INFO - Epoch 7 | Batch 230 | Loss 2.6360 | Acc 0.3275\n",
      "2025-05-24 13:51:38,899 - INFO - Epoch 7 | Batch 240 | Loss 2.6079 | Acc 0.3272\n",
      "2025-05-24 13:51:39,405 - INFO - Epoch 7 | Batch 250 | Loss 2.6555 | Acc 0.3272\n",
      "2025-05-24 13:51:40,004 - INFO - Epoch 7 | Batch 260 | Loss 2.5668 | Acc 0.3272\n",
      "2025-05-24 13:51:40,507 - INFO - Epoch 7 | Batch 270 | Loss 2.6269 | Acc 0.3273\n",
      "2025-05-24 13:51:40,993 - INFO - Epoch 7 | Batch 280 | Loss 2.6240 | Acc 0.3274\n",
      "2025-05-24 13:51:41,498 - INFO - Epoch 7 | Batch 290 | Loss 2.5966 | Acc 0.3281\n",
      "2025-05-24 13:51:41,991 - INFO - Epoch 7 | Batch 300 | Loss 2.6072 | Acc 0.3283\n",
      "2025-05-24 13:51:42,494 - INFO - Epoch 7 | Batch 310 | Loss 2.6087 | Acc 0.3287\n",
      "2025-05-24 13:51:43,020 - INFO - Epoch 7 | Batch 320 | Loss 2.6424 | Acc 0.3290\n",
      "2025-05-24 13:51:43,542 - INFO - Epoch 7 | Batch 330 | Loss 2.6070 | Acc 0.3293\n",
      "2025-05-24 13:51:44,065 - INFO - Epoch 7 | Batch 340 | Loss 2.5970 | Acc 0.3291\n",
      "2025-05-24 13:51:44,580 - INFO - Epoch 7 | Batch 350 | Loss 2.6374 | Acc 0.3289\n",
      "2025-05-24 13:51:45,102 - INFO - Epoch 7 | Batch 360 | Loss 2.6550 | Acc 0.3286\n",
      "2025-05-24 13:51:45,623 - INFO - Epoch 7 | Batch 370 | Loss 2.6644 | Acc 0.3283\n",
      "2025-05-24 13:51:46,141 - INFO - Epoch 7 | Batch 380 | Loss 2.5985 | Acc 0.3280\n",
      "2025-05-24 13:51:46,656 - INFO - Epoch 7 | Batch 390 | Loss 2.5474 | Acc 0.3282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 7/10 — Loss: 0.0071  Acc: 0.3282\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:52:15,289 - INFO - Epoch 8 | Batch 10 | Loss 2.4340 | Acc 0.3680\n",
      "2025-05-24 13:52:15,775 - INFO - Epoch 8 | Batch 20 | Loss 2.5354 | Acc 0.3578\n",
      "2025-05-24 13:52:16,249 - INFO - Epoch 8 | Batch 30 | Loss 2.4891 | Acc 0.3542\n",
      "2025-05-24 13:52:16,722 - INFO - Epoch 8 | Batch 40 | Loss 2.5532 | Acc 0.3527\n",
      "2025-05-24 13:52:17,200 - INFO - Epoch 8 | Batch 50 | Loss 2.5008 | Acc 0.3566\n",
      "2025-05-24 13:52:17,678 - INFO - Epoch 8 | Batch 60 | Loss 2.5355 | Acc 0.3543\n",
      "2025-05-24 13:52:18,156 - INFO - Epoch 8 | Batch 70 | Loss 2.5546 | Acc 0.3526\n",
      "2025-05-24 13:52:18,634 - INFO - Epoch 8 | Batch 80 | Loss 2.5113 | Acc 0.3518\n",
      "2025-05-24 13:52:19,110 - INFO - Epoch 8 | Batch 90 | Loss 2.5080 | Acc 0.3523\n",
      "2025-05-24 13:52:19,589 - INFO - Epoch 8 | Batch 100 | Loss 2.5469 | Acc 0.3516\n",
      "2025-05-24 13:52:20,068 - INFO - Epoch 8 | Batch 110 | Loss 2.4174 | Acc 0.3531\n",
      "2025-05-24 13:52:20,551 - INFO - Epoch 8 | Batch 120 | Loss 2.5730 | Acc 0.3514\n",
      "2025-05-24 13:52:21,030 - INFO - Epoch 8 | Batch 130 | Loss 2.5097 | Acc 0.3515\n",
      "2025-05-24 13:52:21,507 - INFO - Epoch 8 | Batch 140 | Loss 2.5220 | Acc 0.3512\n",
      "2025-05-24 13:52:21,987 - INFO - Epoch 8 | Batch 150 | Loss 2.5361 | Acc 0.3517\n",
      "2025-05-24 13:52:22,467 - INFO - Epoch 8 | Batch 160 | Loss 2.5233 | Acc 0.3512\n",
      "2025-05-24 13:52:22,946 - INFO - Epoch 8 | Batch 170 | Loss 2.5387 | Acc 0.3506\n",
      "2025-05-24 13:52:23,426 - INFO - Epoch 8 | Batch 180 | Loss 2.5581 | Acc 0.3500\n",
      "2025-05-24 13:52:23,903 - INFO - Epoch 8 | Batch 190 | Loss 2.4684 | Acc 0.3513\n",
      "2025-05-24 13:52:24,385 - INFO - Epoch 8 | Batch 200 | Loss 2.4953 | Acc 0.3521\n",
      "2025-05-24 13:52:24,865 - INFO - Epoch 8 | Batch 210 | Loss 2.4757 | Acc 0.3532\n",
      "2025-05-24 13:52:25,346 - INFO - Epoch 8 | Batch 220 | Loss 2.4713 | Acc 0.3532\n",
      "2025-05-24 13:52:25,826 - INFO - Epoch 8 | Batch 230 | Loss 2.5338 | Acc 0.3527\n",
      "2025-05-24 13:52:26,306 - INFO - Epoch 8 | Batch 240 | Loss 2.5760 | Acc 0.3522\n",
      "2025-05-24 13:52:26,785 - INFO - Epoch 8 | Batch 250 | Loss 2.5935 | Acc 0.3515\n",
      "2025-05-24 13:52:27,267 - INFO - Epoch 8 | Batch 260 | Loss 2.4826 | Acc 0.3517\n",
      "2025-05-24 13:52:27,748 - INFO - Epoch 8 | Batch 270 | Loss 2.4247 | Acc 0.3527\n",
      "2025-05-24 13:52:28,231 - INFO - Epoch 8 | Batch 280 | Loss 2.5090 | Acc 0.3521\n",
      "2025-05-24 13:52:28,711 - INFO - Epoch 8 | Batch 290 | Loss 2.5290 | Acc 0.3513\n",
      "2025-05-24 13:52:29,192 - INFO - Epoch 8 | Batch 300 | Loss 2.5627 | Acc 0.3506\n",
      "2025-05-24 13:52:29,673 - INFO - Epoch 8 | Batch 310 | Loss 2.5604 | Acc 0.3500\n",
      "2025-05-24 13:52:30,156 - INFO - Epoch 8 | Batch 320 | Loss 2.5177 | Acc 0.3496\n",
      "2025-05-24 13:52:30,640 - INFO - Epoch 8 | Batch 330 | Loss 2.4903 | Acc 0.3497\n",
      "2025-05-24 13:52:31,123 - INFO - Epoch 8 | Batch 340 | Loss 2.5176 | Acc 0.3498\n",
      "2025-05-24 13:52:31,606 - INFO - Epoch 8 | Batch 350 | Loss 2.5269 | Acc 0.3503\n",
      "2025-05-24 13:52:32,087 - INFO - Epoch 8 | Batch 360 | Loss 2.5151 | Acc 0.3498\n",
      "2025-05-24 13:52:32,572 - INFO - Epoch 8 | Batch 370 | Loss 2.5279 | Acc 0.3500\n",
      "2025-05-24 13:52:33,054 - INFO - Epoch 8 | Batch 380 | Loss 2.5069 | Acc 0.3495\n",
      "2025-05-24 13:52:33,538 - INFO - Epoch 8 | Batch 390 | Loss 2.5310 | Acc 0.3493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 8/10 — Loss: 0.0067  Acc: 0.3493\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:53:02,093 - INFO - Epoch 9 | Batch 10 | Loss 2.4270 | Acc 0.3742\n",
      "2025-05-24 13:53:02,580 - INFO - Epoch 9 | Batch 20 | Loss 2.4281 | Acc 0.3770\n",
      "2025-05-24 13:53:03,055 - INFO - Epoch 9 | Batch 30 | Loss 2.4684 | Acc 0.3682\n",
      "2025-05-24 13:53:03,532 - INFO - Epoch 9 | Batch 40 | Loss 2.3877 | Acc 0.3650\n",
      "2025-05-24 13:53:04,009 - INFO - Epoch 9 | Batch 50 | Loss 2.4126 | Acc 0.3658\n",
      "2025-05-24 13:53:04,485 - INFO - Epoch 9 | Batch 60 | Loss 2.4198 | Acc 0.3693\n",
      "2025-05-24 13:53:04,961 - INFO - Epoch 9 | Batch 70 | Loss 2.3750 | Acc 0.3681\n",
      "2025-05-24 13:53:05,438 - INFO - Epoch 9 | Batch 80 | Loss 2.3205 | Acc 0.3693\n",
      "2025-05-24 13:53:05,919 - INFO - Epoch 9 | Batch 90 | Loss 2.4683 | Acc 0.3694\n",
      "2025-05-24 13:53:06,396 - INFO - Epoch 9 | Batch 100 | Loss 2.4242 | Acc 0.3693\n",
      "2025-05-24 13:53:06,873 - INFO - Epoch 9 | Batch 110 | Loss 2.4003 | Acc 0.3699\n",
      "2025-05-24 13:53:07,352 - INFO - Epoch 9 | Batch 120 | Loss 2.4527 | Acc 0.3690\n",
      "2025-05-24 13:53:07,831 - INFO - Epoch 9 | Batch 130 | Loss 2.4243 | Acc 0.3691\n",
      "2025-05-24 13:53:08,310 - INFO - Epoch 9 | Batch 140 | Loss 2.4450 | Acc 0.3693\n",
      "2025-05-24 13:53:08,790 - INFO - Epoch 9 | Batch 150 | Loss 2.4493 | Acc 0.3693\n",
      "2025-05-24 13:53:09,269 - INFO - Epoch 9 | Batch 160 | Loss 2.4351 | Acc 0.3688\n",
      "2025-05-24 13:53:09,750 - INFO - Epoch 9 | Batch 170 | Loss 2.4396 | Acc 0.3680\n",
      "2025-05-24 13:53:10,228 - INFO - Epoch 9 | Batch 180 | Loss 2.4556 | Acc 0.3688\n",
      "2025-05-24 13:53:10,707 - INFO - Epoch 9 | Batch 190 | Loss 2.3624 | Acc 0.3696\n",
      "2025-05-24 13:53:11,186 - INFO - Epoch 9 | Batch 200 | Loss 2.3644 | Acc 0.3695\n",
      "2025-05-24 13:53:11,665 - INFO - Epoch 9 | Batch 210 | Loss 2.4137 | Acc 0.3701\n",
      "2025-05-24 13:53:12,144 - INFO - Epoch 9 | Batch 220 | Loss 2.5258 | Acc 0.3688\n",
      "2025-05-24 13:53:12,625 - INFO - Epoch 9 | Batch 230 | Loss 2.4387 | Acc 0.3684\n",
      "2025-05-24 13:53:13,105 - INFO - Epoch 9 | Batch 240 | Loss 2.4043 | Acc 0.3686\n",
      "2025-05-24 13:53:13,585 - INFO - Epoch 9 | Batch 250 | Loss 2.4557 | Acc 0.3688\n",
      "2025-05-24 13:53:14,067 - INFO - Epoch 9 | Batch 260 | Loss 2.4599 | Acc 0.3685\n",
      "2025-05-24 13:53:14,546 - INFO - Epoch 9 | Batch 270 | Loss 2.4274 | Acc 0.3684\n",
      "2025-05-24 13:53:15,026 - INFO - Epoch 9 | Batch 280 | Loss 2.3832 | Acc 0.3688\n",
      "2025-05-24 13:53:15,506 - INFO - Epoch 9 | Batch 290 | Loss 2.4099 | Acc 0.3690\n",
      "2025-05-24 13:53:15,989 - INFO - Epoch 9 | Batch 300 | Loss 2.4469 | Acc 0.3691\n",
      "2025-05-24 13:53:16,472 - INFO - Epoch 9 | Batch 310 | Loss 2.4368 | Acc 0.3690\n",
      "2025-05-24 13:53:16,953 - INFO - Epoch 9 | Batch 320 | Loss 2.3574 | Acc 0.3693\n",
      "2025-05-24 13:53:17,434 - INFO - Epoch 9 | Batch 330 | Loss 2.4122 | Acc 0.3688\n",
      "2025-05-24 13:53:17,918 - INFO - Epoch 9 | Batch 340 | Loss 2.3579 | Acc 0.3689\n",
      "2025-05-24 13:53:18,402 - INFO - Epoch 9 | Batch 350 | Loss 2.3835 | Acc 0.3688\n",
      "2025-05-24 13:53:18,886 - INFO - Epoch 9 | Batch 360 | Loss 2.4883 | Acc 0.3682\n",
      "2025-05-24 13:53:19,367 - INFO - Epoch 9 | Batch 370 | Loss 2.4096 | Acc 0.3681\n",
      "2025-05-24 13:53:19,849 - INFO - Epoch 9 | Batch 380 | Loss 2.4630 | Acc 0.3678\n",
      "2025-05-24 13:53:20,332 - INFO - Epoch 9 | Batch 390 | Loss 2.4438 | Acc 0.3679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 9/10 — Loss: 0.0064  Acc: 0.3679\n",
      "1111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:53:49,524 - INFO - Epoch 10 | Batch 10 | Loss 2.3219 | Acc 0.4078\n",
      "2025-05-24 13:53:50,007 - INFO - Epoch 10 | Batch 20 | Loss 2.3248 | Acc 0.4055\n",
      "2025-05-24 13:53:50,490 - INFO - Epoch 10 | Batch 30 | Loss 2.3249 | Acc 0.3971\n",
      "2025-05-24 13:53:50,994 - INFO - Epoch 10 | Batch 40 | Loss 2.4272 | Acc 0.3852\n",
      "2025-05-24 13:53:51,498 - INFO - Epoch 10 | Batch 50 | Loss 2.3819 | Acc 0.3830\n",
      "2025-05-24 13:53:51,979 - INFO - Epoch 10 | Batch 60 | Loss 2.2933 | Acc 0.3844\n",
      "2025-05-24 13:53:52,477 - INFO - Epoch 10 | Batch 70 | Loss 2.3822 | Acc 0.3833\n",
      "2025-05-24 13:53:52,969 - INFO - Epoch 10 | Batch 80 | Loss 2.2764 | Acc 0.3864\n",
      "2025-05-24 13:53:53,449 - INFO - Epoch 10 | Batch 90 | Loss 2.3862 | Acc 0.3855\n",
      "2025-05-24 13:53:53,935 - INFO - Epoch 10 | Batch 100 | Loss 2.3058 | Acc 0.3873\n",
      "2025-05-24 13:53:54,416 - INFO - Epoch 10 | Batch 110 | Loss 2.3383 | Acc 0.3879\n",
      "2025-05-24 13:53:54,911 - INFO - Epoch 10 | Batch 120 | Loss 2.4018 | Acc 0.3868\n",
      "2025-05-24 13:53:55,435 - INFO - Epoch 10 | Batch 130 | Loss 2.3715 | Acc 0.3848\n",
      "2025-05-24 13:53:55,947 - INFO - Epoch 10 | Batch 140 | Loss 2.3202 | Acc 0.3851\n",
      "2025-05-24 13:53:56,450 - INFO - Epoch 10 | Batch 150 | Loss 2.3222 | Acc 0.3849\n",
      "2025-05-24 13:53:56,963 - INFO - Epoch 10 | Batch 160 | Loss 2.3403 | Acc 0.3848\n",
      "2025-05-24 13:53:57,440 - INFO - Epoch 10 | Batch 170 | Loss 2.2173 | Acc 0.3865\n",
      "2025-05-24 13:53:57,917 - INFO - Epoch 10 | Batch 180 | Loss 2.4257 | Acc 0.3855\n",
      "2025-05-24 13:53:58,391 - INFO - Epoch 10 | Batch 190 | Loss 2.3464 | Acc 0.3852\n",
      "2025-05-24 13:53:58,870 - INFO - Epoch 10 | Batch 200 | Loss 2.3385 | Acc 0.3857\n",
      "2025-05-24 13:53:59,344 - INFO - Epoch 10 | Batch 210 | Loss 2.3262 | Acc 0.3862\n",
      "2025-05-24 13:53:59,822 - INFO - Epoch 10 | Batch 220 | Loss 2.3608 | Acc 0.3861\n",
      "2025-05-24 13:54:00,297 - INFO - Epoch 10 | Batch 230 | Loss 2.4365 | Acc 0.3854\n",
      "2025-05-24 13:54:00,778 - INFO - Epoch 10 | Batch 240 | Loss 2.2902 | Acc 0.3861\n",
      "2025-05-24 13:54:01,253 - INFO - Epoch 10 | Batch 250 | Loss 2.2204 | Acc 0.3873\n",
      "2025-05-24 13:54:01,734 - INFO - Epoch 10 | Batch 260 | Loss 2.3356 | Acc 0.3872\n",
      "2025-05-24 13:54:02,210 - INFO - Epoch 10 | Batch 270 | Loss 2.3412 | Acc 0.3871\n",
      "2025-05-24 13:54:02,696 - INFO - Epoch 10 | Batch 280 | Loss 2.3155 | Acc 0.3874\n",
      "2025-05-24 13:54:03,172 - INFO - Epoch 10 | Batch 290 | Loss 2.3140 | Acc 0.3879\n",
      "2025-05-24 13:54:03,686 - INFO - Epoch 10 | Batch 300 | Loss 2.2925 | Acc 0.3879\n",
      "2025-05-24 13:54:04,173 - INFO - Epoch 10 | Batch 310 | Loss 2.3684 | Acc 0.3879\n",
      "2025-05-24 13:54:04,652 - INFO - Epoch 10 | Batch 320 | Loss 2.3906 | Acc 0.3876\n",
      "2025-05-24 13:54:05,128 - INFO - Epoch 10 | Batch 330 | Loss 2.3418 | Acc 0.3878\n",
      "2025-05-24 13:54:05,607 - INFO - Epoch 10 | Batch 340 | Loss 2.3238 | Acc 0.3882\n",
      "2025-05-24 13:54:06,083 - INFO - Epoch 10 | Batch 350 | Loss 2.3920 | Acc 0.3880\n",
      "2025-05-24 13:54:06,562 - INFO - Epoch 10 | Batch 360 | Loss 2.2993 | Acc 0.3883\n",
      "2025-05-24 13:54:07,075 - INFO - Epoch 10 | Batch 370 | Loss 2.3237 | Acc 0.3884\n",
      "2025-05-24 13:54:07,567 - INFO - Epoch 10 | Batch 380 | Loss 2.3224 | Acc 0.3888\n",
      "2025-05-24 13:54:08,045 - INFO - Epoch 10 | Batch 390 | Loss 2.4082 | Acc 0.3886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222\n",
      "333333\n",
      "Epoch 10/10 — Loss: 0.0055  Acc: 0.3886\n",
      "🏃 View run 20250524_134638 at: http://localhost:5000/#/experiments/1/runs/33a739fa31f34f48855b9a1c141e4655\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to end the failed test, avoid RUNNING but failed task jam the workflow\n",
    "mlflow.end_run(\"6ef3867a369b45f3ba0bc1e2e74a4d90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cf3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
