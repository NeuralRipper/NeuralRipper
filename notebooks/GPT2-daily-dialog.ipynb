{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# GPT-2 Daily Dialog Training\n",
    "\n",
    "**Refactored notebook using training_lib**\n",
    "\n",
    "Text generation training with comprehensive MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports", 
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training library\n",
    "import sys\n",
    "sys.path.append('./training_lib')\n",
    "\n",
    "from training_lib import (\n",
    "    create_model, get_conversational_data, create_conversational_dataloader,\n",
    "    UniversalTrainer, get_experiment_logger, print_model_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'gpt2',\n",
    "    'dataset': 'blended_skill_talk',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 5e-5,\n",
    "    'max_conversations': 1000,\n",
    "    'max_length': 512,\n",
    "    'use_mlflow': True\n",
    "}\n",
    "\n",
    "print(\"GPT-2 Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment logger\n",
    "logger = get_experiment_logger(\"GPT2\", \"Conversational\")\n",
    "logger.experiment_start()\n",
    "\n",
    "# Create GPT-2 model and tokenizer\n",
    "model, tokenizer = create_model(CONFIG['model_name'], pretrained=True)\n",
    "print_model_summary(model, CONFIG['model_name'])\n",
    "\n",
    "# Load conversational data\n",
    "logger.info(\"üìö Loading conversational dataset...\")\n",
    "conversations = get_conversational_data(\n",
    "    CONFIG['dataset'], \n",
    "    max_conversations=CONFIG['max_conversations']\n",
    ")\n",
    "\n",
    "# Create data loader\n",
    "train_loader = create_conversational_dataloader(\n",
    "    conversations,\n",
    "    tokenizer,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    max_length=CONFIG['max_length']\n",
    ")\n",
    "\n",
    "logger.data_loaded(len(conversations))\n",
    "print(f\"\\nDataset: {len(conversations):,} conversations, {len(train_loader)} batches\")\n",
    "if conversations:\n",
    "    print(f\"Sample: {conversations[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer for text generation\n",
    "trainer = UniversalTrainer(\n",
    "    model=model,\n",
    "    model_name=\"GPT2\",\n",
    "    dataset_name=\"Conversational\", \n",
    "    epochs=CONFIG['epochs'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    optimizer_name=\"AdamW\",  # Best for GPT-2\n",
    "    use_mlflow=CONFIG['use_mlflow'],\n",
    "    tokenizer=tokenizer,\n",
    "    input_size='max_length_512',\n",
    "    use_pretrained=True,\n",
    "    train_size=len(conversations)\n",
    ")\n",
    "\n",
    "# Start training\n",
    "logger.info(\"üöÄ Starting GPT-2 conversational training...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" STARTING GPT-2 TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = trainer.train(train_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" GPT-2 TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "if summary:\n",
    "    print(\"\\nüìä GPT-2 TRAINING SUMMARY:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"‚úÖ Status: {summary.get('status', 'Unknown')}\")\n",
    "    print(f\"‚è±Ô∏è Total Time: {summary.get('total_time_minutes', 0):.1f} minutes\")\n",
    "    print(f\"üéØ Best Token Accuracy: {summary.get('best_accuracy', 0):.4f}\")\n",
    "    print(f\"üìâ Best Loss: {summary.get('best_loss', float('inf')):.4f}\")\n",
    "    print(f\"üìà Epochs: {summary.get('epochs_completed', 0)}/{CONFIG['epochs']}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "logger.info(\"üéâ GPT-2 conversational training completed!\")"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.9.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 5\n}