{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6d2077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (3.1.1)\n",
      "Requirement already satisfied: transformers in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: mlflow-skinny==3.1.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: Flask<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (1.16.2)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: gunicorn<24 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (23.0.0)\n",
      "Requirement already satisfied: matplotlib<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.10.3)\n",
      "Requirement already satisfied: numpy<3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (2.2.6)\n",
      "Requirement already satisfied: pandas<3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (2.3.0)\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (20.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (1.7.0)\n",
      "Requirement already satisfied: scipy<2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (1.16.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (2.0.41)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.57.0)\n",
      "Requirement already satisfied: fastapi<1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
      "Requirement already satisfied: packaging<26 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.1)\n",
      "Requirement already satisfied: uvicorn<1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
      "Requirement already satisfied: Mako in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.40.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.55b1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
      "Requirement already satisfied: filelock in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yj/.pyenv/versions/ripper/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# ==== IMPORTS ====\n",
    "!pip install mlflow transformers datasets\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import platform\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703c9f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 17:27:29,741 - DEBUG - 666343768.py - PID:26758 - TID:8462606080 - Logger initialization completed\n"
     ]
    }
   ],
   "source": [
    "# ==== LOGGING SETUP ====\n",
    "format_str = '%(asctime)s - %(levelname)s - %(filename)s - PID:%(process)d - TID:%(thread)d - %(message)s'\n",
    "logger = logging.getLogger(__name__ + str(time.time()))\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(format_str))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.debug(\"Logger initialization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac1995d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== COMPREHENSIVE TRAINING MONITOR ====\n",
    "class ComprehensiveTrainingMonitor:\n",
    "    \"\"\"Advanced training monitor with complete metrics tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, criterion, device, model_name='GPT2',\n",
    "                 dataset_name='Conversational', batch_size=16, epochs=10,\n",
    "                 input_size='max_length_512', use_mlflow=True,\n",
    "                 learning_rate=5e-5, use_pretrained=True, train_size=50000,\n",
    "                 val_size=10000, num_workers=0):\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.input_size = input_size\n",
    "        self.use_mlflow = use_mlflow\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Tracking variables\n",
    "        self.best_metric = float('inf')  # Lower perplexity is better\n",
    "        self.epoch_times = []\n",
    "        self.start_time = time.time()\n",
    "        self.run_started = False\n",
    "        self.prev_loss = None  # Track loss improvement\n",
    "\n",
    "        # MLflow configuration\n",
    "        self.mlflow_uri = \"https://neuralripper.com/mlflow/\"\n",
    "        self.gcs_bucket = \"gs://neuralripper-mlflow-artifacts\"\n",
    "\n",
    "        if self.use_mlflow:\n",
    "            self._initialize_mlflow()\n",
    "\n",
    "    def _initialize_mlflow(self):\n",
    "        \"\"\"Initialize MLflow with comprehensive experiment tracking\"\"\"\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(self.mlflow_uri)\n",
    "            mlflow.set_experiment(f\"{self.model_name}-{self.dataset_name}\")\n",
    "\n",
    "            run_name = f\"{self.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            mlflow.start_run(run_name=run_name)\n",
    "            self.run_started = True\n",
    "\n",
    "            params = {\n",
    "                **self._get_model_params(),\n",
    "                **self._get_system_params(),\n",
    "                **self._get_environment_params(),\n",
    "                **self._get_data_params(),\n",
    "                **self._get_training_params(),\n",
    "            }\n",
    "\n",
    "            mlflow.log_params(params)\n",
    "            logger.info(f\"MLflow run started: {run_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize MLflow: {e}\")\n",
    "            self.use_mlflow = False\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get model-specific parameters\"\"\"\n",
    "        params = {\n",
    "            'model_name': self.model_name,\n",
    "            'model_architecture': 'GPT2',\n",
    "            'model_size': 'base',\n",
    "            'use_pretrained': self.use_pretrained,\n",
    "            'num_parameters': sum(p.numel() for p in self.model.parameters()),\n",
    "            'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if hasattr(self.model, 'config'):\n",
    "                params.update({\n",
    "                    'vocab_size': self.model.config.vocab_size,\n",
    "                    'n_positions': self.model.config.n_positions,\n",
    "                    'n_ctx': self.model.config.n_ctx,\n",
    "                    'n_embd': self.model.config.n_embd,\n",
    "                    'n_layer': self.model.config.n_layer,\n",
    "                    'n_head': self.model.config.n_head,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not extract model config: {e}\")\n",
    "            \n",
    "        return params\n",
    "\n",
    "    def _get_system_params(self):\n",
    "        \"\"\"Get system and hardware parameters\"\"\"\n",
    "        return {\n",
    "            'device': str(self.device),\n",
    "            'python_version': platform.python_version(),\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',\n",
    "            'cpu_count': psutil.cpu_count(),\n",
    "            'memory_gb': round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "            'platform': platform.platform(),\n",
    "        }\n",
    "\n",
    "    def _get_environment_params(self):\n",
    "        \"\"\"Get training environment parameters\"\"\"\n",
    "        env_params = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'timezone': str(datetime.now().astimezone().tzinfo),\n",
    "            'random_seed': getattr(torch, 'initial_seed', lambda: None)(),\n",
    "        }\n",
    "        \n",
    "        # Add git information\n",
    "        try:\n",
    "            commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()\n",
    "            branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode().strip()\n",
    "            env_params.update({\n",
    "                'git_commit': commit[:8],\n",
    "                'git_branch': branch,\n",
    "            })\n",
    "        except:\n",
    "            env_params.update({'git_commit': 'unknown', 'git_branch': 'unknown'})\n",
    "            \n",
    "        return env_params\n",
    "\n",
    "    def _get_data_params(self):\n",
    "        \"\"\"Get dataset and data loading parameters\"\"\"\n",
    "        return {\n",
    "            'dataset_name': self.dataset_name,\n",
    "            'train_size': self.train_size,\n",
    "            'val_size': self.val_size,\n",
    "            'batch_size': self.batch_size,\n",
    "            'num_workers': self.num_workers,\n",
    "            'input_size': self.input_size,\n",
    "        }\n",
    "\n",
    "    def _get_training_params(self):\n",
    "        \"\"\"Get training configuration parameters\"\"\"\n",
    "        return {\n",
    "            'epochs': self.epochs,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'optimizer': self.optimizer.__class__.__name__,\n",
    "            'criterion': self.criterion.__class__.__name__,\n",
    "        }\n",
    "\n",
    "    def setup_mlflow(self):\n",
    "        \"\"\"Setup MLflow tracking if not already initialized\"\"\"\n",
    "        if self.use_mlflow and not self.run_started:\n",
    "            self._initialize_mlflow()\n",
    "\n",
    "    def _get_gpu_metrics(self):\n",
    "        \"\"\"Get GPU memory usage metrics\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            return {\n",
    "                'gpu_memory_allocated_mb': torch.cuda.memory_allocated() / 1024**2,\n",
    "                'gpu_memory_reserved_mb': torch.cuda.memory_reserved() / 1024**2,\n",
    "                'gpu_memory_max_allocated_mb': torch.cuda.max_memory_allocated() / 1024**2,\n",
    "            }\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "    def _get_system_metrics(self):\n",
    "        \"\"\"Get system performance metrics\"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            return {\n",
    "                'cpu_percent': psutil.cpu_percent(interval=0.1),\n",
    "                'memory_used_percent': memory.percent,\n",
    "                'memory_used_gb': memory.used / (1024**3),\n",
    "                'memory_available_gb': memory.available / (1024**3),\n",
    "            }\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "    def log_epoch_metrics(self, epoch, epoch_loss, epoch_perplexity, epoch_token_accuracy, batch_count=None, epoch_top5_accuracy=None):\n",
    "        \"\"\"Comprehensive epoch metrics logging for GPT2\"\"\"\n",
    "        if not self.use_mlflow or not self.run_started:\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "\n",
    "            # Timing metrics\n",
    "            epoch_time = current_time - (self.start_time if epoch == 0 else self.start_time + sum(self.epoch_times))\n",
    "            self.epoch_times.append(epoch_time)\n",
    "\n",
    "            # Core metrics - MAP TO FRONTEND EXPECTED NAMES\n",
    "            metrics = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": epoch_loss,           # Frontend expects 'train_loss'\n",
    "                \"train_accuracy\": epoch_token_accuracy,  # Frontend expects 'train_accuracy' \n",
    "                \"perplexity\": epoch_perplexity,     # Text model specific metric\n",
    "                \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
    "                \"epoch_time_seconds\": epoch_time,\n",
    "                \"total_time_seconds\": current_time - self.start_time,\n",
    "                \"avg_epoch_time\": sum(self.epoch_times) / len(self.epoch_times),\n",
    "            }\n",
    "\n",
    "            # Add top-5 accuracy if provided (text models only)\n",
    "            if epoch_top5_accuracy is not None:\n",
    "                metrics[\"top5_accuracy\"] = epoch_top5_accuracy\n",
    "\n",
    "            # Training dynamics (use loss for improvement tracking)\n",
    "            if self.prev_loss is not None:\n",
    "                loss_improvement = self.prev_loss - epoch_loss\n",
    "                metrics.update({\n",
    "                    \"loss_improvement\": loss_improvement,\n",
    "                    \"loss_improvement_percent\": (loss_improvement / self.prev_loss) * 100 if self.prev_loss != 0 else 0,\n",
    "                })\n",
    "            \n",
    "            self.prev_loss = epoch_loss\n",
    "\n",
    "            # Performance metrics\n",
    "            if batch_count and epoch_time > 0:\n",
    "                samples_per_sec = (self.batch_size * batch_count) / epoch_time\n",
    "                metrics.update({\n",
    "                    \"batches_per_second\": batch_count / epoch_time,\n",
    "                    \"samples_per_second\": samples_per_sec,\n",
    "                })\n",
    "\n",
    "            # GPU/System metrics\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_metrics = self._get_gpu_metrics()\n",
    "                metrics.update(gpu_metrics)\n",
    "\n",
    "            system_metrics = self._get_system_metrics()\n",
    "            metrics.update(system_metrics)\n",
    "\n",
    "            mlflow.log_metrics(metrics, step=epoch)\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log epoch metrics: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def should_log_model(self, current_metric, metric_name=\"token_accuracy\"):\n",
    "        \"\"\"Enhanced model checkpointing with improvement tracking\"\"\"\n",
    "        if current_metric > self.best_metric:  # Higher token accuracy is better\n",
    "            improvement = current_metric - self.best_metric\n",
    "            self.best_metric = current_metric\n",
    "            \n",
    "            logger.info(f\"New best {metric_name}: {current_metric:.4f} \"\n",
    "                    f\"(improvement: +{improvement:.4f})\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def log_model_artifact(self):\n",
    "        \"\"\"Log model metadata to MLflow\"\"\"\n",
    "        if not self.use_mlflow:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            model_info = {\n",
    "                'model_architecture': self.model_name,\n",
    "                'best_perplexity': float(self.best_metric),\n",
    "                'total_parameters': sum(p.numel() for p in self.model.parameters()),\n",
    "                'model_size_mb': sum(p.numel() * p.element_size() for p in self.model.parameters()) / (1024**2),\n",
    "            }\n",
    "            mlflow.log_dict(model_info, \"model_metadata.json\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log model metadata: {e}\")\n",
    "\n",
    "    def end_run(self, status=\"FINISHED\"):\n",
    "        \"\"\"End MLflow run and return summary\"\"\"\n",
    "        if not self.use_mlflow:\n",
    "            return {}\n",
    "\n",
    "        total_time = time.time() - self.start_time\n",
    "        summary = {\n",
    "            'final_total_training_time_minutes': round(total_time / 60, 2),\n",
    "            'final_best_perplexity': float(self.best_metric),\n",
    "            'final_epochs_completed': len(self.epoch_times),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            mlflow.log_params({'training_status': status})\n",
    "            mlflow.log_metrics(summary)\n",
    "            mlflow.end_run()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to end MLflow run: {e}\")\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ed8764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: dict_keys(['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'])\n",
      "Sample item: {'personas': [\"i've 2 kids.\", 'i love flowers.'], 'additional_context': '', 'previous_utterance': [\"I love live music, that's why I try to go to concerts\", 'I do too. Wat do you like?'], 'context': 'empathetic_dialogues', 'free_messages': ['I like acting, I hope to be an actor, what about you?', 'No, but someday.', 'After I am done with school I plan to have a family.', 'I hope so, how old are your kids?', 'I would imagine. I am sure they a great kids.', 'I wish I had more time to do stuff like that. Medical school is exhausting. '], 'guided_messages': ['that is ok.  have any kids?', 'that is good. I have 2', 'that is great! you will be ready', '5 & 7.  they take up a lot of my time', 'luckily, they love flowers just as much as I do.  we spend a lot of time in the garden', 'sounds like it. have you gotten any acting jobs, though?'], 'suggestions': {'convai2': [\"i love acting ! i'll be famous someday . what do you do ?\", 'no no kids , might get some though . one day', 'that is great . i am going to a concert later', '15 and 17 , two boys sooo fun', 'they really are . and a handful at times', 'it can be sometimes . i bet being a doctor is a lot of work too .'], 'empathetic_dialogues': ['Any favorite actors?', 'One day.', 'How long must you attend school?', '4 and 5 and I have a teenager', 'They are most of the time!', \"Oh. I don't know how medical school works. I am studying srt history.\"], 'wizard_of_wikipedia': ['I would like to develop my acting skills. What are some tips you have to not get nervous?', 'I will still wimp out. i want to be famous like the rolling stones  though.', 'good', \"Close to 30! I just always have to put in a ton of work when mother's day comes around haha\", 'They are actually very good with kids!', 'yeah but there are a lot of programs that help!']}, 'guided_chosen_suggestions': ['', '', '', '', '', ''], 'label_candidates': []}\n",
      "Training conversations: 1000\n",
      "Sample conversation: User: that is ok.  have any kids?\n",
      "Assistant: that is good. I have 2<|endoftext|>...\n"
     ]
    }
   ],
   "source": [
    "# ==== DATA LOADING ====\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load conversational dataset and prepare for training\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load blended_skill_talk dataset\n",
    "        dataset = load_dataset(\"blended_skill_talk\")\n",
    "        conversations = []\n",
    "        \n",
    "        # Debug: print first item structure\n",
    "        print(\"Dataset structure:\", dataset[\"train\"][0].keys())\n",
    "        print(\"Sample item:\", dataset[\"train\"][0])\n",
    "        \n",
    "        for item in dataset[\"train\"]:\n",
    "            # Extract the guided and free messages\n",
    "            guided_messages = item.get(\"guided_messages\", [])\n",
    "            free_messages = item.get(\"free_messages\", [])\n",
    "            \n",
    "            # Create conversations from guided messages\n",
    "            if guided_messages and len(guided_messages) >= 2:\n",
    "                for i in range(0, len(guided_messages) - 1, 2):\n",
    "                    if i + 1 < len(guided_messages):\n",
    "                        user_msg = guided_messages[i].strip()\n",
    "                        assistant_msg = guided_messages[i + 1].strip()\n",
    "                        if user_msg and assistant_msg:\n",
    "                            conversation = f\"User: {user_msg}\\nAssistant: {assistant_msg}<|endoftext|>\"\n",
    "                            conversations.append(conversation)\n",
    "            \n",
    "            # Create conversations from free messages  \n",
    "            if free_messages and len(free_messages) >= 2:\n",
    "                for i in range(0, len(free_messages) - 1, 2):\n",
    "                    if i + 1 < len(free_messages):\n",
    "                        user_msg = free_messages[i].strip()\n",
    "                        assistant_msg = free_messages[i + 1].strip()\n",
    "                        if user_msg and assistant_msg:\n",
    "                            conversation = f\"User: {user_msg}\\nAssistant: {assistant_msg}<|endoftext|>\"\n",
    "                            conversations.append(conversation)\n",
    "            \n",
    "            if len(conversations) >= 1000:  # Stop early for demo\n",
    "                break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return []\n",
    "    \n",
    "    conversations = conversations[:1000]\n",
    "    \n",
    "    print(f\"Training conversations: {len(conversations)}\")\n",
    "    if conversations:\n",
    "        print(f\"Sample conversation: {conversations[0][:200]}...\")\n",
    "    else:\n",
    "        print(\"No conversations found - check dataset structure\")\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Load data\n",
    "train_conversations = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bf74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATA LOADER ====\n",
    "def create_data_loader(conversations, tokenizer, batch_size=16, max_length=512):\n",
    "    \"\"\"Create DataLoader with tokenization for conversations\"\"\"\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        # Tokenize all conversations in batch\n",
    "        encoded = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # For GPT-2, input_ids serve as both input and labels (shifted by 1)\n",
    "        input_ids = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone()  # For language modeling\n",
    "        }\n",
    "    \n",
    "    return DataLoader(\n",
    "        conversations,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6c7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ENHANCED GPT-2 CLASS ====\n",
    "class EnhancedGPT2:\n",
    "    \"\"\"GPT-2 with comprehensive monitoring integration\"\"\"\n",
    "\n",
    "    def __init__(self, num_epochs=3, batch_size=16, \n",
    "                 learning_rate=5e-5, use_mlflow=True, model_name='gpt2'):\n",
    "        \n",
    "        # Core parameters\n",
    "        self._use_mlflow = use_mlflow\n",
    "        self._batch_size = batch_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._learning_rate = learning_rate\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Create model and tokenizer\n",
    "        self._tokenizer = self._create_tokenizer()\n",
    "        self._model = self._create_model()\n",
    "        self._device = self._set_device()\n",
    "        self._model.to(self._device)\n",
    "        self._criterion = self._set_criterion()\n",
    "        self._optimizer = self._set_optimizer()\n",
    "\n",
    "        # Initialize comprehensive monitor\n",
    "        self.monitor = ComprehensiveTrainingMonitor(\n",
    "            model=self._model,\n",
    "            optimizer=self._optimizer,\n",
    "            criterion=self._criterion,\n",
    "            device=self._device,\n",
    "            model_name='GPT2',\n",
    "            dataset_name='Conversational',\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            input_size='max_length_512',\n",
    "            use_mlflow=use_mlflow,\n",
    "            use_pretrained=True,\n",
    "            train_size=10000,\n",
    "            val_size=2000,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Model initialized on device: {self._device}\")\n",
    "        logger.info(f\"Model parameters: {sum(p.numel() for p in self._model.parameters()):,}\")\n",
    "\n",
    "    def _create_tokenizer(self):\n",
    "        \"\"\"Create GPT-2 tokenizer\"\"\"\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have pad token\n",
    "        return tokenizer\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create GPT-2 model for language modeling\"\"\"\n",
    "        return GPT2LMHeadModel.from_pretrained(self.model_name)\n",
    "\n",
    "    def _set_device(self):\n",
    "        \"\"\"Set appropriate device for training\"\"\"\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    def _set_optimizer(self):\n",
    "        \"\"\"Configure AdamW optimizer\"\"\"\n",
    "        return torch.optim.AdamW(self._model.parameters(), lr=self._learning_rate)\n",
    "\n",
    "    def _set_criterion(self):\n",
    "        \"\"\"Set loss function for language modeling\"\"\"\n",
    "        return nn.CrossEntropyLoss(ignore_index=self._tokenizer.pad_token_id)\n",
    "\n",
    "    def train_epoch(self, data_loader, epoch_idx):\n",
    "        \"\"\"Enhanced training epoch with comprehensive monitoring including top-k accuracy\"\"\"\n",
    "        logger.info(f\"Starting epoch {epoch_idx+1}, total batches: {len(data_loader)}\")\n",
    "\n",
    "        self._model.train()\n",
    "        epoch_total_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_top5_correct = 0\n",
    "        running_total = 0\n",
    "        batch_count = len(data_loader)\n",
    "\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(self._device)\n",
    "            attention_mask = batch['attention_mask'].to(self._device)\n",
    "            labels = batch['labels'].to(self._device)\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            \n",
    "            # GPT-2 forward pass\n",
    "            outputs = self._model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            # Calculate token-level accuracy\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            # Mask out padding tokens for accuracy calculation\n",
    "            mask = (labels != self._tokenizer.pad_token_id)\n",
    "            correct_predictions = ((predictions == labels) & mask).sum().item()\n",
    "            total_tokens = mask.sum().item()\n",
    "            \n",
    "            # Calculate top-5 accuracy\n",
    "            top5_preds = torch.topk(logits, 5, dim=-1)[1]  # Shape: [batch, seq_len, 5]\n",
    "            top5_correct = 0\n",
    "            for k in range(5):\n",
    "                top5_correct += ((top5_preds[:, :, k] == labels) & mask).sum().item()\n",
    "            \n",
    "            running_correct += correct_predictions\n",
    "            running_top5_correct += top5_correct\n",
    "            running_total += total_tokens\n",
    "            running_loss += loss.item()\n",
    "            epoch_total_loss += loss.item()\n",
    "\n",
    "            # Progress logging every 10 batches\n",
    "            if idx % 10 == 9:\n",
    "                avg_loss = running_loss / 10\n",
    "                perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "                token_accuracy = running_correct / running_total if running_total > 0 else 0\n",
    "                top5_accuracy = running_top5_correct / running_total if running_total > 0 else 0\n",
    "                logger.info(f\"Epoch {epoch_idx + 1} | Batch {idx + 1} | \"\n",
    "                        f\"Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f} | \"\n",
    "                        f\"Token Acc: {token_accuracy:.4f} | Top5 Acc: {top5_accuracy:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = epoch_total_loss / batch_count\n",
    "        epoch_perplexity = torch.exp(torch.tensor(epoch_loss)).item()\n",
    "        epoch_token_accuracy = running_correct / running_total if running_total > 0 else 0\n",
    "        epoch_top5_accuracy = running_top5_correct / running_total if running_total > 0 else 0\n",
    "\n",
    "        logger.info(f\"Epoch {epoch_idx + 1} completed - Loss: {epoch_loss:.4f}, \"\n",
    "                f\"Perplexity: {epoch_perplexity:.2f}, Token Accuracy: {epoch_token_accuracy:.4f}, \"\n",
    "                f\"Top5 Accuracy: {epoch_top5_accuracy:.4f}\")\n",
    "        \n",
    "        return epoch_loss, epoch_perplexity, epoch_token_accuracy, epoch_top5_accuracy, batch_count\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        \"\"\"Enhanced training with comprehensive monitoring\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting training for {self._num_epochs} epochs on {self._device}\")\n",
    "\n",
    "            # Setup MLflow\n",
    "            self.monitor.setup_mlflow()\n",
    "\n",
    "            for epoch in range(self._num_epochs):\n",
    "                epoch_loss, epoch_perplexity, epoch_token_accuracy, epoch_top5_accuracy, batch_count = self.train_epoch(train_loader, epoch)\n",
    "\n",
    "                # Log comprehensive epoch metrics - UPDATE THIS CALL\n",
    "                metrics = self.monitor.log_epoch_metrics(\n",
    "                    epoch, epoch_loss, epoch_perplexity, epoch_token_accuracy, \n",
    "                    batch_count, epoch_top5_accuracy  # Add top5 parameter\n",
    "                )\n",
    "\n",
    "                # Model checkpointing (lower perplexity is better)\n",
    "                if self.monitor.should_log_model(epoch_perplexity):\n",
    "                    self.monitor.log_model_artifact()\n",
    "                    logger.info(f\"New best model metadata saved with perplexity: {epoch_perplexity:.2f}\")\n",
    "\n",
    "                # Enhanced progress display\n",
    "                print(f\"Epoch {epoch+1}/{self._num_epochs}: \"\n",
    "                    f\"Loss: {epoch_loss:.4f} | Perplexity: {epoch_perplexity:.2f} | \"\n",
    "                    f\"Token Acc: {epoch_token_accuracy:.4f} | Top5 Acc: {epoch_top5_accuracy:.4f} | \"\n",
    "                    f\"Time: {metrics.get('epoch_time_seconds', 0):.1f}s | \"\n",
    "                    f\"LR: {metrics.get('learning_rate', 0):.2e} | \"\n",
    "                    f\"Memory: {metrics.get('memory_used_percent', 0):.1f}%\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed: {e}\")\n",
    "            self.monitor.end_run(status=\"FAILED\")\n",
    "            raise\n",
    "        finally:\n",
    "            summary = self.monitor.end_run()\n",
    "            logger.info(f\"Training completed. Summary: {summary}\")\n",
    "            print(f\"\\nTraining Summary:\")\n",
    "            print(f\"Total time: {summary.get('final_total_training_time_minutes', 0):.1f} minutes\")\n",
    "            print(f\"Best perplexity: {summary.get('final_best_perplexity', 0):.2f}\")\n",
    "            print(f\"Epochs completed: {summary.get('final_epochs_completed', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a6568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 17:27:51,309 - INFO - 268818953.py - PID:26758 - TID:8462606080 - Initializing Enhanced GPT-2 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/27 17:27:53 INFO mlflow.tracking.fluent: Experiment with name 'GPT2-Conversational' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 17:27:54,140 - INFO - 2929732547.py - PID:26758 - TID:8462606080 - MLflow run started: GPT2_20250727_172753\n",
      "2025-07-27 17:27:54,141 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Model initialized on device: mps\n",
      "2025-07-27 17:27:54,141 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Model parameters: 124,439,808\n",
      "Model initialized successfully!\n",
      "Device: mps\n",
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n",
      "Number of batches per epoch: 16\n",
      "Total samples per epoch: 512\n",
      "\n",
      "==================================================\n",
      " STARTING ENHANCED GPT-2 TRAINING\n",
      "==================================================\n",
      "Dataset: Daily Dialog (Conversational)\n",
      "Model: GPT-2 (pretrained)\n",
      "Epochs: 20\n",
      "Batch size: 64\n",
      "Learning rate: 5e-05\n",
      "MLflow tracking: Enabled\n",
      "MLflow URI: https://neuralripper.com/mlflow/\n",
      "==================================================\n",
      "\n",
      "2025-07-27 17:27:54,142 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Starting training for 20 epochs on mps\n",
      "2025-07-27 17:27:54,143 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Starting epoch 1, total batches: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 17:30:35,890 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Epoch 1 | Batch 10 | Loss: 3.5898 | Perplexity: 36.23 | Token Acc: 0.0159 | Top5 Acc: 0.0363\n",
      "2025-07-27 17:32:38,796 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Epoch 1 completed - Loss: 2.9995, Perplexity: 20.08, Token Accuracy: 0.0114, Top5 Accuracy: 0.0353\n",
      "Epoch 1/20: Loss: 2.9995 | Perplexity: 20.08 | Token Acc: 0.0114 | Top5 Acc: 0.0353 | Time: 285.9s | LR: 5.00e-05 | Memory: 90.0%\n",
      "2025-07-27 17:32:43,302 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Starting epoch 2, total batches: 16\n",
      "2025-07-27 17:35:48,643 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Epoch 2 | Batch 10 | Loss: 1.9218 | Perplexity: 6.83 | Token Acc: 0.0027 | Top5 Acc: 0.0157\n",
      "2025-07-27 17:37:12,035 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Epoch 2 completed - Loss: 1.8578, Perplexity: 6.41, Token Accuracy: 0.0023, Top5 Accuracy: 0.0139\n",
      "Epoch 2/20: Loss: 1.8578 | Perplexity: 6.41 | Token Acc: 0.0023 | Top5 Acc: 0.0139 | Time: 273.2s | LR: 5.00e-05 | Memory: 87.1%\n",
      "2025-07-27 17:37:13,103 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Starting epoch 3, total batches: 16\n",
      " View run GPT2_20250727_172753 at: https://neuralripper.com/mlflow/#/experiments/3/runs/919094331cb0479db7c0a7881e49f084\n",
      " View experiment at: https://neuralripper.com/mlflow/#/experiments/3\n",
      "2025-07-27 17:40:44,645 - INFO - 3216512036.py - PID:26758 - TID:8462606080 - Training completed. Summary: {'final_total_training_time_minutes': 12.77, 'final_best_perplexity': inf, 'final_epochs_completed': 2}\n",
      "\n",
      "Training Summary:\n",
      "Total time: 12.8 minutes\n",
      "Best perplexity: inf\n",
      "Epochs completed: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 152\u001b[0m, in \u001b[0;36mEnhancedGPT2.train\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;241m.\u001b[39msetup_mlflow()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_epochs):\n\u001b[0;32m--> 152\u001b[0m     epoch_loss, epoch_perplexity, epoch_token_accuracy, epoch_top5_accuracy, batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Log comprehensive epoch metrics - UPDATE THIS CALL\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;241m.\u001b[39mlog_epoch_metrics(\n\u001b[1;32m    156\u001b[0m         epoch, epoch_loss, epoch_perplexity, epoch_token_accuracy, \n\u001b[1;32m    157\u001b[0m         batch_count, epoch_top5_accuracy  \u001b[38;5;66;03m# Add top5 parameter\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[6], line 98\u001b[0m, in \u001b[0;36mEnhancedGPT2.train_epoch\u001b[0;34m(self, data_loader, epoch_idx)\u001b[0m\n\u001b[1;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     96\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 98\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Calculate token-level accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/ripper/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ripper/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ripper/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==== INITIALIZE AND TRAIN ====\n",
    "logger.info(\"Initializing Enhanced GPT-2 model\")\n",
    "\n",
    "model = EnhancedGPT2(\n",
    "    num_epochs=10,\n",
    "    batch_size=64,\n",
    "    learning_rate=5e-5,\n",
    "    use_mlflow=True,\n",
    "    model_name='gpt2'\n",
    ")\n",
    "\n",
    "print(\"Model initialized successfully!\")\n",
    "print(f\"Device: {model._device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model._model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model._model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Create data loader\n",
    "train_loader = create_data_loader(\n",
    "    train_conversations, \n",
    "    model._tokenizer, \n",
    "    batch_size=64,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"Number of batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Total samples per epoch: {len(train_loader) * 32}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" STARTING ENHANCED GPT-2 TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: Daily Dialog (Conversational)\")\n",
    "print(f\"Model: GPT-2 (pretrained)\")\n",
    "print(f\"Epochs: {model._num_epochs}\")\n",
    "print(f\"Batch size: {model._batch_size}\")\n",
    "print(f\"Learning rate: {model._learning_rate}\")\n",
    "print(f\"MLflow tracking: {'Enabled' if model._use_mlflow else 'Disabled'}\")\n",
    "print(f\"MLflow URI: {model.monitor.mlflow_uri}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Start training\n",
    "model.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d3df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa5cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2105f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ripper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
