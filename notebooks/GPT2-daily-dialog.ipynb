{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6d2077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (3.1.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: mlflow-skinny==3.1.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: Flask<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (1.16.2)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: gunicorn<24 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (23.0.0)\n",
      "Requirement already satisfied: matplotlib<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (3.10.3)\n",
      "Requirement already satisfied: numpy<3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (2.2.6)\n",
      "Requirement already satisfied: pandas<3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (2.3.0)\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (20.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (1.7.0)\n",
      "Requirement already satisfied: scipy<2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (1.16.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow) (2.0.41)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle<4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.57.0)\n",
      "Requirement already satisfied: fastapi<1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
      "Requirement already satisfied: packaging<26 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.1)\n",
      "Requirement already satisfied: uvicorn<1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
      "Requirement already satisfied: Mako in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.40.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from matplotlib<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.55b1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
      "Requirement already satisfied: filelock in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.14-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yj/.pyenv/versions/3.11.13/envs/ripper/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Using cached transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "Using cached huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading aiohttp-3.12.14-cp311-cp311-macosx_11_0_arm64.whl (470 kB)\n",
      "Downloading multidict-6.6.3-cp311-cp311-macosx_11_0_arm64.whl (44 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl (89 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl (47 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, safetensors, regex, propcache, multidict, hf-xet, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [datasets]/18\u001b[0m [datasets]ers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.5 huggingface-hub-0.33.4 multidict-6.6.3 multiprocess-0.70.16 propcache-0.3.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.2 transformers-4.53.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yj/.pyenv/versions/ripper/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# ==== IMPORTS ====\n",
    "!pip install mlflow transformers datasets\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import platform\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703c9f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-19 17:35:33,616 - DEBUG - 666343768.py - PID:58260 - TID:8462606080 - Logger initialization completed\n"
     ]
    }
   ],
   "source": [
    "# ==== LOGGING SETUP ====\n",
    "format_str = '%(asctime)s - %(levelname)s - %(filename)s - PID:%(process)d - TID:%(thread)d - %(message)s'\n",
    "logger = logging.getLogger(__name__ + str(time.time()))\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(format_str))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.debug(\"Logger initialization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac1995d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== COMPREHENSIVE TRAINING MONITOR ====\n",
    "class ComprehensiveTrainingMonitor:\n",
    "    \"\"\"Advanced training monitor with complete metrics tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, criterion, device, model_name='GPT2',\n",
    "                 dataset_name='Conversational', batch_size=16, epochs=10,\n",
    "                 input_size='max_length_512', use_mlflow=True,\n",
    "                 learning_rate=5e-5, use_pretrained=True, train_size=50000,\n",
    "                 val_size=10000, num_workers=0):\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.input_size = input_size\n",
    "        self.use_mlflow = use_mlflow\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Tracking variables\n",
    "        self.best_metric = float('inf')  # Lower perplexity is better\n",
    "        self.epoch_times = []\n",
    "        self.start_time = time.time()\n",
    "        self.run_started = False\n",
    "        self.prev_loss = None  # Track loss improvement\n",
    "\n",
    "        # MLflow configuration\n",
    "        self.mlflow_uri = \"https://mlflow-server-631028107267.us-central1.run.app/\"\n",
    "        self.gcs_bucket = \"gs://neuralripper-mlflow-artifacts\"\n",
    "\n",
    "        if self.use_mlflow:\n",
    "            self._initialize_mlflow()\n",
    "\n",
    "    def _initialize_mlflow(self):\n",
    "        \"\"\"Initialize MLflow with comprehensive experiment tracking\"\"\"\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(self.mlflow_uri)\n",
    "            mlflow.set_experiment(f\"{self.model_name}-{self.dataset_name}\")\n",
    "\n",
    "            run_name = f\"{self.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            mlflow.start_run(run_name=run_name)\n",
    "            self.run_started = True\n",
    "\n",
    "            params = {\n",
    "                **self._get_model_params(),\n",
    "                **self._get_system_params(),\n",
    "                **self._get_environment_params(),\n",
    "                **self._get_data_params(),\n",
    "                **self._get_training_params(),\n",
    "            }\n",
    "\n",
    "            mlflow.log_params(params)\n",
    "            logger.info(f\"MLflow run started: {run_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize MLflow: {e}\")\n",
    "            self.use_mlflow = False\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get model-specific parameters\"\"\"\n",
    "        params = {\n",
    "            'model_name': self.model_name,\n",
    "            'model_architecture': 'GPT2',\n",
    "            'model_size': 'base',\n",
    "            'use_pretrained': self.use_pretrained,\n",
    "            'num_parameters': sum(p.numel() for p in self.model.parameters()),\n",
    "            'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if hasattr(self.model, 'config'):\n",
    "                params.update({\n",
    "                    'vocab_size': self.model.config.vocab_size,\n",
    "                    'n_positions': self.model.config.n_positions,\n",
    "                    'n_ctx': self.model.config.n_ctx,\n",
    "                    'n_embd': self.model.config.n_embd,\n",
    "                    'n_layer': self.model.config.n_layer,\n",
    "                    'n_head': self.model.config.n_head,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not extract model config: {e}\")\n",
    "            \n",
    "        return params\n",
    "\n",
    "    def _get_system_params(self):\n",
    "        \"\"\"Get system and hardware parameters\"\"\"\n",
    "        return {\n",
    "            'device': str(self.device),\n",
    "            'python_version': platform.python_version(),\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',\n",
    "            'cpu_count': psutil.cpu_count(),\n",
    "            'memory_gb': round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "            'platform': platform.platform(),\n",
    "        }\n",
    "\n",
    "    def _get_environment_params(self):\n",
    "        \"\"\"Get training environment parameters\"\"\"\n",
    "        env_params = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'timezone': str(datetime.now().astimezone().tzinfo),\n",
    "            'random_seed': getattr(torch, 'initial_seed', lambda: None)(),\n",
    "        }\n",
    "        \n",
    "        # Add git information\n",
    "        try:\n",
    "            commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()\n",
    "            branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode().strip()\n",
    "            env_params.update({\n",
    "                'git_commit': commit[:8],\n",
    "                'git_branch': branch,\n",
    "            })\n",
    "        except:\n",
    "            env_params.update({'git_commit': 'unknown', 'git_branch': 'unknown'})\n",
    "            \n",
    "        return env_params\n",
    "\n",
    "    def _get_data_params(self):\n",
    "        \"\"\"Get dataset and data loading parameters\"\"\"\n",
    "        return {\n",
    "            'dataset_name': self.dataset_name,\n",
    "            'train_size': self.train_size,\n",
    "            'val_size': self.val_size,\n",
    "            'batch_size': self.batch_size,\n",
    "            'num_workers': self.num_workers,\n",
    "            'input_size': self.input_size,\n",
    "        }\n",
    "\n",
    "    def _get_training_params(self):\n",
    "        \"\"\"Get training configuration parameters\"\"\"\n",
    "        return {\n",
    "            'epochs': self.epochs,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'optimizer': self.optimizer.__class__.__name__,\n",
    "            'criterion': self.criterion.__class__.__name__,\n",
    "        }\n",
    "\n",
    "    def setup_mlflow(self):\n",
    "        \"\"\"Setup MLflow tracking if not already initialized\"\"\"\n",
    "        if self.use_mlflow and not self.run_started:\n",
    "            self._initialize_mlflow()\n",
    "\n",
    "    def _get_gpu_metrics(self):\n",
    "        \"\"\"Get GPU memory usage metrics\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            return {\n",
    "                'gpu_memory_allocated_mb': torch.cuda.memory_allocated() / 1024**2,\n",
    "                'gpu_memory_reserved_mb': torch.cuda.memory_reserved() / 1024**2,\n",
    "                'gpu_memory_max_allocated_mb': torch.cuda.max_memory_allocated() / 1024**2,\n",
    "            }\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "    def _get_system_metrics(self):\n",
    "        \"\"\"Get system performance metrics\"\"\"\n",
    "        try:\n",
    "            memory = psutil.virtual_memory()\n",
    "            return {\n",
    "                'cpu_percent': psutil.cpu_percent(interval=0.1),\n",
    "                'memory_used_percent': memory.percent,\n",
    "                'memory_used_gb': memory.used / (1024**3),\n",
    "                'memory_available_gb': memory.available / (1024**3),\n",
    "            }\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "    def log_epoch_metrics(self, epoch, epoch_loss, epoch_perplexity, epoch_token_accuracy, batch_count=None, epoch_top5_accuracy=None):\n",
    "        \"\"\"Comprehensive epoch metrics logging for GPT2\"\"\"\n",
    "        if not self.use_mlflow or not self.run_started:\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "\n",
    "            # Timing metrics\n",
    "            epoch_time = current_time - (self.start_time if epoch == 0 else self.start_time + sum(self.epoch_times))\n",
    "            self.epoch_times.append(epoch_time)\n",
    "\n",
    "            # Core metrics - MAP TO FRONTEND EXPECTED NAMES\n",
    "            metrics = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": epoch_loss,           # Frontend expects 'train_loss'\n",
    "                \"train_accuracy\": epoch_token_accuracy,  # Frontend expects 'train_accuracy' \n",
    "                \"perplexity\": epoch_perplexity,     # Text model specific metric\n",
    "                \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
    "                \"epoch_time_seconds\": epoch_time,\n",
    "                \"total_time_seconds\": current_time - self.start_time,\n",
    "                \"avg_epoch_time\": sum(self.epoch_times) / len(self.epoch_times),\n",
    "            }\n",
    "\n",
    "            # Add top-5 accuracy if provided (text models only)\n",
    "            if epoch_top5_accuracy is not None:\n",
    "                metrics[\"top5_accuracy\"] = epoch_top5_accuracy\n",
    "\n",
    "            # Training dynamics (use loss for improvement tracking)\n",
    "            if self.prev_loss is not None:\n",
    "                loss_improvement = self.prev_loss - epoch_loss\n",
    "                metrics.update({\n",
    "                    \"loss_improvement\": loss_improvement,\n",
    "                    \"loss_improvement_percent\": (loss_improvement / self.prev_loss) * 100 if self.prev_loss != 0 else 0,\n",
    "                })\n",
    "            \n",
    "            self.prev_loss = epoch_loss\n",
    "\n",
    "            # Performance metrics\n",
    "            if batch_count and epoch_time > 0:\n",
    "                samples_per_sec = (self.batch_size * batch_count) / epoch_time\n",
    "                metrics.update({\n",
    "                    \"batches_per_second\": batch_count / epoch_time,\n",
    "                    \"samples_per_second\": samples_per_sec,\n",
    "                })\n",
    "\n",
    "            # GPU/System metrics\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_metrics = self._get_gpu_metrics()\n",
    "                metrics.update(gpu_metrics)\n",
    "\n",
    "            system_metrics = self._get_system_metrics()\n",
    "            metrics.update(system_metrics)\n",
    "\n",
    "            mlflow.log_metrics(metrics, step=epoch)\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log epoch metrics: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def should_log_model(self, current_metric, metric_name=\"token_accuracy\"):\n",
    "        \"\"\"Enhanced model checkpointing with improvement tracking\"\"\"\n",
    "        if current_metric > self.best_metric:  # Higher token accuracy is better\n",
    "            improvement = current_metric - self.best_metric\n",
    "            self.best_metric = current_metric\n",
    "            \n",
    "            logger.info(f\"New best {metric_name}: {current_metric:.4f} \"\n",
    "                    f\"(improvement: +{improvement:.4f})\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def log_model_artifact(self):\n",
    "        \"\"\"Log model metadata to MLflow\"\"\"\n",
    "        if not self.use_mlflow:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            model_info = {\n",
    "                'model_architecture': self.model_name,\n",
    "                'best_perplexity': float(self.best_metric),\n",
    "                'total_parameters': sum(p.numel() for p in self.model.parameters()),\n",
    "                'model_size_mb': sum(p.numel() * p.element_size() for p in self.model.parameters()) / (1024**2),\n",
    "            }\n",
    "            mlflow.log_dict(model_info, \"model_metadata.json\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log model metadata: {e}\")\n",
    "\n",
    "    def end_run(self, status=\"FINISHED\"):\n",
    "        \"\"\"End MLflow run and return summary\"\"\"\n",
    "        if not self.use_mlflow:\n",
    "            return {}\n",
    "\n",
    "        total_time = time.time() - self.start_time\n",
    "        summary = {\n",
    "            'final_total_training_time_minutes': round(total_time / 60, 2),\n",
    "            'final_best_perplexity': float(self.best_metric),\n",
    "            'final_epochs_completed': len(self.epoch_times),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            mlflow.log_params({'training_status': status})\n",
    "            mlflow.log_metrics(summary)\n",
    "            mlflow.end_run()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to end MLflow run: {e}\")\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ed8764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: dict_keys(['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'])\n",
      "Sample item: {'personas': [\"i've 2 kids.\", 'i love flowers.'], 'additional_context': '', 'previous_utterance': [\"I love live music, that's why I try to go to concerts\", 'I do too. Wat do you like?'], 'context': 'empathetic_dialogues', 'free_messages': ['I like acting, I hope to be an actor, what about you?', 'No, but someday.', 'After I am done with school I plan to have a family.', 'I hope so, how old are your kids?', 'I would imagine. I am sure they a great kids.', 'I wish I had more time to do stuff like that. Medical school is exhausting. '], 'guided_messages': ['that is ok.  have any kids?', 'that is good. I have 2', 'that is great! you will be ready', '5 & 7.  they take up a lot of my time', 'luckily, they love flowers just as much as I do.  we spend a lot of time in the garden', 'sounds like it. have you gotten any acting jobs, though?'], 'suggestions': {'convai2': [\"i love acting ! i'll be famous someday . what do you do ?\", 'no no kids , might get some though . one day', 'that is great . i am going to a concert later', '15 and 17 , two boys sooo fun', 'they really are . and a handful at times', 'it can be sometimes . i bet being a doctor is a lot of work too .'], 'empathetic_dialogues': ['Any favorite actors?', 'One day.', 'How long must you attend school?', '4 and 5 and I have a teenager', 'They are most of the time!', \"Oh. I don't know how medical school works. I am studying srt history.\"], 'wizard_of_wikipedia': ['I would like to develop my acting skills. What are some tips you have to not get nervous?', 'I will still wimp out. i want to be famous like the rolling stones  though.', 'good', \"Close to 30! I just always have to put in a ton of work when mother's day comes around haha\", 'They are actually very good with kids!', 'yeah but there are a lot of programs that help!']}, 'guided_chosen_suggestions': ['', '', '', '', '', ''], 'label_candidates': []}\n",
      "Training conversations: 1000\n",
      "Sample conversation: User: that is ok.  have any kids?\n",
      "Assistant: that is good. I have 2<|endoftext|>...\n"
     ]
    }
   ],
   "source": [
    "# ==== DATA LOADING ====\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load conversational dataset and prepare for training\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load blended_skill_talk dataset\n",
    "        dataset = load_dataset(\"blended_skill_talk\")\n",
    "        conversations = []\n",
    "        \n",
    "        # Debug: print first item structure\n",
    "        print(\"Dataset structure:\", dataset[\"train\"][0].keys())\n",
    "        print(\"Sample item:\", dataset[\"train\"][0])\n",
    "        \n",
    "        for item in dataset[\"train\"]:\n",
    "            # Extract the guided and free messages\n",
    "            guided_messages = item.get(\"guided_messages\", [])\n",
    "            free_messages = item.get(\"free_messages\", [])\n",
    "            \n",
    "            # Create conversations from guided messages\n",
    "            if guided_messages and len(guided_messages) >= 2:\n",
    "                for i in range(0, len(guided_messages) - 1, 2):\n",
    "                    if i + 1 < len(guided_messages):\n",
    "                        user_msg = guided_messages[i].strip()\n",
    "                        assistant_msg = guided_messages[i + 1].strip()\n",
    "                        if user_msg and assistant_msg:\n",
    "                            conversation = f\"User: {user_msg}\\nAssistant: {assistant_msg}<|endoftext|>\"\n",
    "                            conversations.append(conversation)\n",
    "            \n",
    "            # Create conversations from free messages  \n",
    "            if free_messages and len(free_messages) >= 2:\n",
    "                for i in range(0, len(free_messages) - 1, 2):\n",
    "                    if i + 1 < len(free_messages):\n",
    "                        user_msg = free_messages[i].strip()\n",
    "                        assistant_msg = free_messages[i + 1].strip()\n",
    "                        if user_msg and assistant_msg:\n",
    "                            conversation = f\"User: {user_msg}\\nAssistant: {assistant_msg}<|endoftext|>\"\n",
    "                            conversations.append(conversation)\n",
    "            \n",
    "            if len(conversations) >= 1000:  # Stop early for demo\n",
    "                break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return []\n",
    "    \n",
    "    conversations = conversations[:1000]\n",
    "    \n",
    "    print(f\"Training conversations: {len(conversations)}\")\n",
    "    if conversations:\n",
    "        print(f\"Sample conversation: {conversations[0][:200]}...\")\n",
    "    else:\n",
    "        print(\"No conversations found - check dataset structure\")\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Load data\n",
    "train_conversations = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7bf74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATA LOADER ====\n",
    "def create_data_loader(conversations, tokenizer, batch_size=16, max_length=512):\n",
    "    \"\"\"Create DataLoader with tokenization for conversations\"\"\"\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        # Tokenize all conversations in batch\n",
    "        encoded = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # For GPT-2, input_ids serve as both input and labels (shifted by 1)\n",
    "        input_ids = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone()  # For language modeling\n",
    "        }\n",
    "    \n",
    "    return DataLoader(\n",
    "        conversations,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b6c7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ENHANCED GPT-2 CLASS ====\n",
    "class EnhancedGPT2:\n",
    "    \"\"\"GPT-2 with comprehensive monitoring integration\"\"\"\n",
    "\n",
    "    def __init__(self, num_epochs=3, batch_size=16, \n",
    "                 learning_rate=5e-5, use_mlflow=True, model_name='gpt2'):\n",
    "        \n",
    "        # Core parameters\n",
    "        self._use_mlflow = use_mlflow\n",
    "        self._batch_size = batch_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._learning_rate = learning_rate\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Create model and tokenizer\n",
    "        self._tokenizer = self._create_tokenizer()\n",
    "        self._model = self._create_model()\n",
    "        self._device = self._set_device()\n",
    "        self._model.to(self._device)\n",
    "        self._criterion = self._set_criterion()\n",
    "        self._optimizer = self._set_optimizer()\n",
    "\n",
    "        # Initialize comprehensive monitor\n",
    "        self.monitor = ComprehensiveTrainingMonitor(\n",
    "            model=self._model,\n",
    "            optimizer=self._optimizer,\n",
    "            criterion=self._criterion,\n",
    "            device=self._device,\n",
    "            model_name='GPT2',\n",
    "            dataset_name='Conversational',\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            input_size='max_length_512',\n",
    "            use_mlflow=use_mlflow,\n",
    "            use_pretrained=True,\n",
    "            train_size=10000,\n",
    "            val_size=2000,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Model initialized on device: {self._device}\")\n",
    "        logger.info(f\"Model parameters: {sum(p.numel() for p in self._model.parameters()):,}\")\n",
    "\n",
    "    def _create_tokenizer(self):\n",
    "        \"\"\"Create GPT-2 tokenizer\"\"\"\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have pad token\n",
    "        return tokenizer\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create GPT-2 model for language modeling\"\"\"\n",
    "        return GPT2LMHeadModel.from_pretrained(self.model_name)\n",
    "\n",
    "    def _set_device(self):\n",
    "        \"\"\"Set appropriate device for training\"\"\"\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    def _set_optimizer(self):\n",
    "        \"\"\"Configure AdamW optimizer\"\"\"\n",
    "        return torch.optim.AdamW(self._model.parameters(), lr=self._learning_rate)\n",
    "\n",
    "    def _set_criterion(self):\n",
    "        \"\"\"Set loss function for language modeling\"\"\"\n",
    "        return nn.CrossEntropyLoss(ignore_index=self._tokenizer.pad_token_id)\n",
    "\n",
    "    def train_epoch(self, data_loader, epoch_idx):\n",
    "        \"\"\"Enhanced training epoch with comprehensive monitoring including top-k accuracy\"\"\"\n",
    "        logger.info(f\"Starting epoch {epoch_idx+1}, total batches: {len(data_loader)}\")\n",
    "\n",
    "        self._model.train()\n",
    "        epoch_total_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_top5_correct = 0\n",
    "        running_total = 0\n",
    "        batch_count = len(data_loader)\n",
    "\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(self._device)\n",
    "            attention_mask = batch['attention_mask'].to(self._device)\n",
    "            labels = batch['labels'].to(self._device)\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            \n",
    "            # GPT-2 forward pass\n",
    "            outputs = self._model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            # Calculate token-level accuracy\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            # Mask out padding tokens for accuracy calculation\n",
    "            mask = (labels != self._tokenizer.pad_token_id)\n",
    "            correct_predictions = ((predictions == labels) & mask).sum().item()\n",
    "            total_tokens = mask.sum().item()\n",
    "            \n",
    "            # Calculate top-5 accuracy\n",
    "            top5_preds = torch.topk(logits, 5, dim=-1)[1]  # Shape: [batch, seq_len, 5]\n",
    "            top5_correct = 0\n",
    "            for k in range(5):\n",
    "                top5_correct += ((top5_preds[:, :, k] == labels) & mask).sum().item()\n",
    "            \n",
    "            running_correct += correct_predictions\n",
    "            running_top5_correct += top5_correct\n",
    "            running_total += total_tokens\n",
    "            running_loss += loss.item()\n",
    "            epoch_total_loss += loss.item()\n",
    "\n",
    "            # Progress logging every 10 batches\n",
    "            if idx % 10 == 9:\n",
    "                avg_loss = running_loss / 10\n",
    "                perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "                token_accuracy = running_correct / running_total if running_total > 0 else 0\n",
    "                top5_accuracy = running_top5_correct / running_total if running_total > 0 else 0\n",
    "                logger.info(f\"Epoch {epoch_idx + 1} | Batch {idx + 1} | \"\n",
    "                        f\"Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f} | \"\n",
    "                        f\"Token Acc: {token_accuracy:.4f} | Top5 Acc: {top5_accuracy:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = epoch_total_loss / batch_count\n",
    "        epoch_perplexity = torch.exp(torch.tensor(epoch_loss)).item()\n",
    "        epoch_token_accuracy = running_correct / running_total if running_total > 0 else 0\n",
    "        epoch_top5_accuracy = running_top5_correct / running_total if running_total > 0 else 0\n",
    "\n",
    "        logger.info(f\"Epoch {epoch_idx + 1} completed - Loss: {epoch_loss:.4f}, \"\n",
    "                f\"Perplexity: {epoch_perplexity:.2f}, Token Accuracy: {epoch_token_accuracy:.4f}, \"\n",
    "                f\"Top5 Accuracy: {epoch_top5_accuracy:.4f}\")\n",
    "        \n",
    "        return epoch_loss, epoch_perplexity, epoch_token_accuracy, epoch_top5_accuracy, batch_count\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        \"\"\"Enhanced training with comprehensive monitoring\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting training for {self._num_epochs} epochs on {self._device}\")\n",
    "\n",
    "            # Setup MLflow\n",
    "            self.monitor.setup_mlflow()\n",
    "\n",
    "            for epoch in range(self._num_epochs):\n",
    "                epoch_loss, epoch_perplexity, epoch_token_accuracy, epoch_top5_accuracy, batch_count = self.train_epoch(train_loader, epoch)\n",
    "\n",
    "                # Log comprehensive epoch metrics - UPDATE THIS CALL\n",
    "                metrics = self.monitor.log_epoch_metrics(\n",
    "                    epoch, epoch_loss, epoch_perplexity, epoch_token_accuracy, \n",
    "                    batch_count, epoch_top5_accuracy  # Add top5 parameter\n",
    "                )\n",
    "\n",
    "                # Model checkpointing (lower perplexity is better)\n",
    "                if self.monitor.should_log_model(epoch_perplexity):\n",
    "                    self.monitor.log_model_artifact()\n",
    "                    logger.info(f\"New best model metadata saved with perplexity: {epoch_perplexity:.2f}\")\n",
    "\n",
    "                # Enhanced progress display\n",
    "                print(f\"Epoch {epoch+1}/{self._num_epochs}: \"\n",
    "                    f\"Loss: {epoch_loss:.4f} | Perplexity: {epoch_perplexity:.2f} | \"\n",
    "                    f\"Token Acc: {epoch_token_accuracy:.4f} | Top5 Acc: {epoch_top5_accuracy:.4f} | \"\n",
    "                    f\"Time: {metrics.get('epoch_time_seconds', 0):.1f}s | \"\n",
    "                    f\"LR: {metrics.get('learning_rate', 0):.2e} | \"\n",
    "                    f\"Memory: {metrics.get('memory_used_percent', 0):.1f}%\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed: {e}\")\n",
    "            self.monitor.end_run(status=\"FAILED\")\n",
    "            raise\n",
    "        finally:\n",
    "            summary = self.monitor.end_run()\n",
    "            logger.info(f\"Training completed. Summary: {summary}\")\n",
    "            print(f\"\\nTraining Summary:\")\n",
    "            print(f\"Total time: {summary.get('final_total_training_time_minutes', 0):.1f} minutes\")\n",
    "            print(f\"Best perplexity: {summary.get('final_best_perplexity', 0):.2f}\")\n",
    "            print(f\"Epochs completed: {summary.get('final_epochs_completed', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60a6568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-20 02:35:47,189 - INFO - 268818953.py - PID:58260 - TID:8462606080 - Initializing Enhanced GPT-2 model\n",
      "2025-07-20 02:35:53,010 - INFO - 1505081118.py - PID:58260 - TID:8462606080 - MLflow run started: GPT2_20250720_023550\n",
      "2025-07-20 02:35:53,012 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Model initialized on device: mps\n",
      "2025-07-20 02:35:53,012 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Model parameters: 124,439,808\n",
      "Model initialized successfully!\n",
      "Device: mps\n",
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n",
      "Number of batches per epoch: 16\n",
      "Total samples per epoch: 512\n",
      "\n",
      "==================================================\n",
      " STARTING ENHANCED GPT-2 TRAINING\n",
      "==================================================\n",
      "Dataset: Daily Dialog (Conversational)\n",
      "Model: GPT-2 (pretrained)\n",
      "Epochs: 20\n",
      "Batch size: 64\n",
      "Learning rate: 5e-05\n",
      "MLflow tracking: Enabled\n",
      "MLflow URI: https://mlflow-server-631028107267.us-central1.run.app/\n",
      "==================================================\n",
      "\n",
      "2025-07-20 02:35:53,070 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting training for 20 epochs on mps\n",
      "2025-07-20 02:35:53,070 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 1, total batches: 16\n",
      "2025-07-20 02:36:40,474 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 1 | Batch 10 | Loss: 3.5700 | Perplexity: 35.52 | Token Acc: 0.0153 | Top5 Acc: 0.0367\n",
      "2025-07-20 02:36:58,458 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 1 completed - Loss: 3.0243, Perplexity: 20.58, Token Accuracy: 0.0108, Top5 Accuracy: 0.0358\n",
      "Epoch 1/20: Loss: 3.0243 | Perplexity: 20.58 | Token Acc: 0.0108 | Top5 Acc: 0.0358 | Time: 68.0s | LR: 5.00e-05 | Memory: 88.6%\n",
      "2025-07-20 02:37:00,968 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 2, total batches: 16\n",
      "2025-07-20 02:37:31,479 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 2 | Batch 10 | Loss: 1.8872 | Perplexity: 6.60 | Token Acc: 0.0027 | Top5 Acc: 0.0159\n",
      "2025-07-20 02:37:48,780 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 2 completed - Loss: 1.8462, Perplexity: 6.34, Token Accuracy: 0.0024, Top5 Accuracy: 0.0147\n",
      "Epoch 2/20: Loss: 1.8462 | Perplexity: 6.34 | Token Acc: 0.0024 | Top5 Acc: 0.0147 | Time: 50.3s | LR: 5.00e-05 | Memory: 86.7%\n",
      "2025-07-20 02:37:51,158 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 3, total batches: 16\n",
      "2025-07-20 02:38:30,464 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 3 | Batch 10 | Loss: 1.7227 | Perplexity: 5.60 | Token Acc: 0.0006 | Top5 Acc: 0.0092\n",
      "2025-07-20 02:38:58,549 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 3 completed - Loss: 1.7288, Perplexity: 5.63, Token Accuracy: 0.0006, Top5 Accuracy: 0.0093\n",
      "Epoch 3/20: Loss: 1.7288 | Perplexity: 5.63 | Token Acc: 0.0006 | Top5 Acc: 0.0093 | Time: 69.8s | LR: 5.00e-05 | Memory: 88.8%\n",
      "2025-07-20 02:39:00,793 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 4, total batches: 16\n",
      "2025-07-20 02:39:59,126 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 4 | Batch 10 | Loss: 1.6535 | Perplexity: 5.23 | Token Acc: 0.0003 | Top5 Acc: 0.0081\n",
      "2025-07-20 02:40:25,020 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 4 completed - Loss: 1.6643, Perplexity: 5.28, Token Accuracy: 0.0003, Top5 Accuracy: 0.0081\n",
      "Epoch 4/20: Loss: 1.6643 | Perplexity: 5.28 | Token Acc: 0.0003 | Top5 Acc: 0.0081 | Time: 86.5s | LR: 5.00e-05 | Memory: 87.9%\n",
      "2025-07-20 02:40:27,366 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 5, total batches: 16\n",
      "2025-07-20 02:41:17,767 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 5 | Batch 10 | Loss: 1.6653 | Perplexity: 5.29 | Token Acc: 0.0003 | Top5 Acc: 0.0089\n",
      "2025-07-20 02:41:43,948 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 5 completed - Loss: 1.6367, Perplexity: 5.14, Token Accuracy: 0.0003, Top5 Accuracy: 0.0088\n",
      "Epoch 5/20: Loss: 1.6367 | Perplexity: 5.14 | Token Acc: 0.0003 | Top5 Acc: 0.0088 | Time: 78.9s | LR: 5.00e-05 | Memory: 80.8%\n",
      "2025-07-20 02:41:45,990 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 6, total batches: 16\n",
      "2025-07-20 02:42:50,936 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 6 | Batch 10 | Loss: 1.5539 | Perplexity: 4.73 | Token Acc: 0.0003 | Top5 Acc: 0.0071\n",
      "2025-07-20 02:43:23,158 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 6 completed - Loss: 1.5899, Perplexity: 4.90, Token Accuracy: 0.0003, Top5 Accuracy: 0.0074\n",
      "Epoch 6/20: Loss: 1.5899 | Perplexity: 4.90 | Token Acc: 0.0003 | Top5 Acc: 0.0074 | Time: 99.2s | LR: 5.00e-05 | Memory: 82.9%\n",
      "2025-07-20 02:43:25,545 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 7, total batches: 16\n",
      "2025-07-20 02:44:12,290 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 7 | Batch 10 | Loss: 1.5610 | Perplexity: 4.76 | Token Acc: 0.0003 | Top5 Acc: 0.0077\n",
      "2025-07-20 02:44:39,469 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 7 completed - Loss: 1.5589, Perplexity: 4.75, Token Accuracy: 0.0003, Top5 Accuracy: 0.0072\n",
      "Epoch 7/20: Loss: 1.5589 | Perplexity: 4.75 | Token Acc: 0.0003 | Top5 Acc: 0.0072 | Time: 76.3s | LR: 5.00e-05 | Memory: 85.5%\n",
      "2025-07-20 02:44:41,734 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 8, total batches: 16\n",
      "2025-07-20 02:45:31,131 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 8 | Batch 10 | Loss: 1.5367 | Perplexity: 4.65 | Token Acc: 0.0003 | Top5 Acc: 0.0060\n",
      "2025-07-20 02:46:11,229 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 8 completed - Loss: 1.5212, Perplexity: 4.58, Token Accuracy: 0.0004, Top5 Accuracy: 0.0066\n",
      "Epoch 8/20: Loss: 1.5212 | Perplexity: 4.58 | Token Acc: 0.0004 | Top5 Acc: 0.0066 | Time: 91.8s | LR: 5.00e-05 | Memory: 85.0%\n",
      "2025-07-20 02:46:13,561 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 9, total batches: 16\n",
      "2025-07-20 02:47:05,450 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 9 | Batch 10 | Loss: 1.4886 | Perplexity: 4.43 | Token Acc: 0.0003 | Top5 Acc: 0.0063\n",
      "2025-07-20 02:47:36,602 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 9 completed - Loss: 1.4993, Perplexity: 4.48, Token Accuracy: 0.0003, Top5 Accuracy: 0.0063\n",
      "Epoch 9/20: Loss: 1.4993 | Perplexity: 4.48 | Token Acc: 0.0003 | Top5 Acc: 0.0063 | Time: 85.4s | LR: 5.00e-05 | Memory: 86.2%\n",
      "2025-07-20 02:47:38,811 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 10, total batches: 16\n",
      "2025-07-20 02:48:27,910 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 10 | Batch 10 | Loss: 1.4921 | Perplexity: 4.45 | Token Acc: 0.0004 | Top5 Acc: 0.0072\n",
      "2025-07-20 02:49:01,371 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 10 completed - Loss: 1.4867, Perplexity: 4.42, Token Accuracy: 0.0003, Top5 Accuracy: 0.0068\n",
      "Epoch 10/20: Loss: 1.4867 | Perplexity: 4.42 | Token Acc: 0.0003 | Top5 Acc: 0.0068 | Time: 84.8s | LR: 5.00e-05 | Memory: 84.2%\n",
      "2025-07-20 02:49:03,700 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 11, total batches: 16\n",
      "2025-07-20 02:50:02,700 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 11 | Batch 10 | Loss: 1.4027 | Perplexity: 4.07 | Token Acc: 0.0003 | Top5 Acc: 0.0078\n",
      "2025-07-20 02:50:33,648 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 11 completed - Loss: 1.4177, Perplexity: 4.13, Token Accuracy: 0.0003, Top5 Accuracy: 0.0077\n",
      "Epoch 11/20: Loss: 1.4177 | Perplexity: 4.13 | Token Acc: 0.0003 | Top5 Acc: 0.0077 | Time: 92.3s | LR: 5.00e-05 | Memory: 82.1%\n",
      "2025-07-20 02:50:36,017 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 12, total batches: 16\n",
      "2025-07-20 02:51:28,853 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 12 | Batch 10 | Loss: 1.3490 | Perplexity: 3.85 | Token Acc: 0.0003 | Top5 Acc: 0.0071\n",
      "2025-07-20 02:51:57,294 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 12 completed - Loss: 1.3844, Perplexity: 3.99, Token Accuracy: 0.0002, Top5 Accuracy: 0.0069\n",
      "Epoch 12/20: Loss: 1.3844 | Perplexity: 3.99 | Token Acc: 0.0002 | Top5 Acc: 0.0069 | Time: 83.6s | LR: 5.00e-05 | Memory: 87.4%\n",
      "2025-07-20 02:51:59,805 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 13, total batches: 16\n",
      "2025-07-20 02:52:47,588 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 13 | Batch 10 | Loss: 1.4198 | Perplexity: 4.14 | Token Acc: 0.0003 | Top5 Acc: 0.0075\n",
      "2025-07-20 02:53:19,113 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 13 completed - Loss: 1.3830, Perplexity: 3.99, Token Accuracy: 0.0002, Top5 Accuracy: 0.0071\n",
      "Epoch 13/20: Loss: 1.3830 | Perplexity: 3.99 | Token Acc: 0.0002 | Top5 Acc: 0.0071 | Time: 81.8s | LR: 5.00e-05 | Memory: 83.8%\n",
      "2025-07-20 02:53:21,279 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 14, total batches: 16\n",
      "2025-07-20 02:54:11,089 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 14 | Batch 10 | Loss: 1.3145 | Perplexity: 3.72 | Token Acc: 0.0002 | Top5 Acc: 0.0077\n",
      "2025-07-20 02:54:37,435 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 14 completed - Loss: 1.3514, Perplexity: 3.86, Token Accuracy: 0.0002, Top5 Accuracy: 0.0077\n",
      "Epoch 14/20: Loss: 1.3514 | Perplexity: 3.86 | Token Acc: 0.0002 | Top5 Acc: 0.0077 | Time: 78.3s | LR: 5.00e-05 | Memory: 85.1%\n",
      "2025-07-20 02:54:39,710 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 15, total batches: 16\n",
      "2025-07-20 02:55:34,769 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 15 | Batch 10 | Loss: 1.2775 | Perplexity: 3.59 | Token Acc: 0.0002 | Top5 Acc: 0.0076\n",
      "2025-07-20 02:56:22,596 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 15 completed - Loss: 1.2892, Perplexity: 3.63, Token Accuracy: 0.0003, Top5 Accuracy: 0.0073\n",
      "Epoch 15/20: Loss: 1.2892 | Perplexity: 3.63 | Token Acc: 0.0003 | Top5 Acc: 0.0073 | Time: 105.2s | LR: 5.00e-05 | Memory: 82.6%\n",
      "2025-07-20 02:56:24,913 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 16, total batches: 16\n",
      "2025-07-20 02:57:33,977 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 16 | Batch 10 | Loss: 1.2691 | Perplexity: 3.56 | Token Acc: 0.0002 | Top5 Acc: 0.0072\n",
      "2025-07-20 02:58:01,869 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 16 completed - Loss: 1.2568, Perplexity: 3.51, Token Accuracy: 0.0003, Top5 Accuracy: 0.0070\n",
      "Epoch 16/20: Loss: 1.2568 | Perplexity: 3.51 | Token Acc: 0.0003 | Top5 Acc: 0.0070 | Time: 99.3s | LR: 5.00e-05 | Memory: 82.3%\n",
      "2025-07-20 02:58:04,277 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 17, total batches: 16\n",
      "2025-07-20 02:58:48,422 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 17 | Batch 10 | Loss: 1.2505 | Perplexity: 3.49 | Token Acc: 0.0003 | Top5 Acc: 0.0073\n",
      "2025-07-20 02:59:14,417 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 17 completed - Loss: 1.2361, Perplexity: 3.44, Token Accuracy: 0.0003, Top5 Accuracy: 0.0071\n",
      "Epoch 17/20: Loss: 1.2361 | Perplexity: 3.44 | Token Acc: 0.0003 | Top5 Acc: 0.0071 | Time: 72.5s | LR: 5.00e-05 | Memory: 83.5%\n",
      "2025-07-20 02:59:16,621 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 18, total batches: 16\n",
      "2025-07-20 02:59:58,260 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 18 | Batch 10 | Loss: 1.1871 | Perplexity: 3.28 | Token Acc: 0.0002 | Top5 Acc: 0.0070\n",
      "2025-07-20 03:00:21,171 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 18 completed - Loss: 1.2109, Perplexity: 3.36, Token Accuracy: 0.0003, Top5 Accuracy: 0.0069\n",
      "Epoch 18/20: Loss: 1.2109 | Perplexity: 3.36 | Token Acc: 0.0003 | Top5 Acc: 0.0069 | Time: 66.8s | LR: 5.00e-05 | Memory: 85.4%\n",
      "2025-07-20 03:00:23,486 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 19, total batches: 16\n",
      "2025-07-20 03:01:18,823 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 19 | Batch 10 | Loss: 1.1310 | Perplexity: 3.10 | Token Acc: 0.0001 | Top5 Acc: 0.0066\n",
      "2025-07-20 03:01:47,142 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 19 completed - Loss: 1.1487, Perplexity: 3.15, Token Accuracy: 0.0002, Top5 Accuracy: 0.0071\n",
      "Epoch 19/20: Loss: 1.1487 | Perplexity: 3.15 | Token Acc: 0.0002 | Top5 Acc: 0.0071 | Time: 86.0s | LR: 5.00e-05 | Memory: 85.3%\n",
      "2025-07-20 03:01:49,500 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Starting epoch 20, total batches: 16\n",
      "2025-07-20 03:02:43,295 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 20 | Batch 10 | Loss: 1.0998 | Perplexity: 3.00 | Token Acc: 0.0001 | Top5 Acc: 0.0067\n",
      "2025-07-20 03:03:21,324 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Epoch 20 completed - Loss: 1.1189, Perplexity: 3.06, Token Accuracy: 0.0002, Top5 Accuracy: 0.0074\n",
      "Epoch 20/20: Loss: 1.1189 | Perplexity: 3.06 | Token Acc: 0.0002 | Top5 Acc: 0.0074 | Time: 94.2s | LR: 5.00e-05 | Memory: 81.9%\n",
      "🏃 View run GPT2_20250720_023550 at: https://mlflow-server-631028107267.us-central1.run.app/#/experiments/5/runs/ee08620941dd45b098b92c2076ee47c4\n",
      "🧪 View experiment at: https://mlflow-server-631028107267.us-central1.run.app/#/experiments/5\n",
      "2025-07-20 03:03:28,626 - INFO - 3216512036.py - PID:58260 - TID:8462606080 - Training completed. Summary: {'final_total_training_time_minutes': 27.56, 'final_best_perplexity': inf, 'final_epochs_completed': 20}\n",
      "\n",
      "Training Summary:\n",
      "Total time: 27.6 minutes\n",
      "Best perplexity: inf\n",
      "Epochs completed: 20\n"
     ]
    }
   ],
   "source": [
    "# ==== INITIALIZE AND TRAIN ====\n",
    "logger.info(\"Initializing Enhanced GPT-2 model\")\n",
    "\n",
    "model = EnhancedGPT2(\n",
    "    num_epochs=20,\n",
    "    batch_size=64,\n",
    "    learning_rate=5e-5,\n",
    "    use_mlflow=True,\n",
    "    model_name='gpt2'\n",
    ")\n",
    "\n",
    "print(\"Model initialized successfully!\")\n",
    "print(f\"Device: {model._device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model._model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model._model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Create data loader\n",
    "train_loader = create_data_loader(\n",
    "    train_conversations, \n",
    "    model._tokenizer, \n",
    "    batch_size=64,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"Number of batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Total samples per epoch: {len(train_loader) * 32}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" STARTING ENHANCED GPT-2 TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: Daily Dialog (Conversational)\")\n",
    "print(f\"Model: GPT-2 (pretrained)\")\n",
    "print(f\"Epochs: {model._num_epochs}\")\n",
    "print(f\"Batch size: {model._batch_size}\")\n",
    "print(f\"Learning rate: {model._learning_rate}\")\n",
    "print(f\"MLflow tracking: {'Enabled' if model._use_mlflow else 'Disabled'}\")\n",
    "print(f\"MLflow URI: {model.monitor.mlflow_uri}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Start training\n",
    "model.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d3df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa5cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2105f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ripper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
