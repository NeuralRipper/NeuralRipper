{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T04:57:16.241636Z",
     "start_time": "2025-06-23T04:57:15.841382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "# Step 1: Data Pipeline, Preprocessing the data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "data_root = project_root / \"data\"\n",
    "\n",
    "# Transform of data, resize, normalize, etc\n",
    "# Compose accept a list of instance\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(224),  # upsample the CIFAR from 32x32 to 224x224, the EfficientNet expects this\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Dataset and pytorch Dataloader\n",
    "train_ds = CIFAR100(\n",
    "    root=data_root,         # the path stores actual data\n",
    "    train=True,             # create dataset from training set, otherwise from testset\n",
    "    transform=train_tf\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "indices = random.sample(range(len(train_ds)), 10000)\n",
    "train_ds_small = Subset(train_ds, indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    # train_ds,\n",
    "    train_ds_small,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,       # MPS and multiprocessing don't work well together, might slow down, use 0\n",
    "    pin_memory=False     # Important for MPS       TODO: understand this\n",
    ")\n",
    "\n",
    "print(f\"[ALL]Training dataset size: {len(train_ds)}\")\n",
    "print(f\"[SMALL]Training dataset size: {len(train_ds_small)}\")"
   ],
   "id": "9699ba139611dd5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ALL]Training dataset size: 50000\n",
      "[SMALL]Training dataset size: 10000\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T04:57:17.038081Z",
     "start_time": "2025-06-23T04:57:17.009702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up loggings\n",
    "import logging, time\n",
    "\n",
    "format = '%(asctime)s - %(levelname)s - %(filename)s - PID:%(process)d - TID:%(thread)d - %(message)s'\n",
    "logger = logging.getLogger(__name__ + str(time.time()))     # in jupyter notebook, avoid accumulating multiple loggers\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False  # create a logger with getLogger(__name__), it sends messages to both your custom handler AND the root logger by default. Setting propagate = False stops this behavior.\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(format))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.debug(\"Logger Initialization Completed\")"
   ],
   "id": "15eee5d99f0e1464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 21:57:17,036 - DEBUG - 2310805002.py - PID:61797 - TID:8386830080 - Logger Initialization Completed\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:01:22.713951Z",
     "start_time": "2025-06-23T05:01:22.703514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.backends.mps\n",
    "\n",
    "# Step 2: Model definition\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "class MyEfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Complexity is lowest for B0\n",
    "    \"Model\",\"ϕ\\phiϕ\",\"Resolution\",\"Parameters (M)\",\"FLOPs (B)\",\"Top-1 Accuracy (ImageNet)\"\n",
    "    \"EfficientNet-B0\",\"0\",\"224x224\",\"5.3\",\"0.39\",\"~77.1%\"\n",
    "    \"EfficientNet-B1\",\"1\",\"240x240\",\"7.8\",\"0.70\",\"~79.1%\"\n",
    "    \"EfficientNet-B2\",\"2\",\"260x260\",\"9.2\",\"1.0\",\"~80.1%\"\n",
    "    \"EfficientNet-B3\",\"3\",\"300x300\",\"12\",\"1.8\",\"~81.6%\"\n",
    "    \"EfficientNet-B4\",\"4\",\"380x380\",\"19\",\"4.2\",\"~82.9%\"\n",
    "    \"EfficientNet-B5\",\"5\",\"456x456\",\"30\",\"9.9\",\"~83.7%\"\n",
    "    \"EfficientNet-B6\",\"6\",\"528x528\",\"43\",\"19\",\"~84.0%\"\n",
    "    \"EfficientNet-B7\",\"7\",\"600x600\",\"66\",\"37\",\"~84.3%\"\n",
    "    # fine-tuning with pretrained weights, use lr=1e-4, too big is not good for fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size=32, num_epochs=10, num_classes=100, learning_rate=1e-4, use_mlflow=True):\n",
    "        super().__init__()\n",
    "        self._num_classes = num_classes\n",
    "        self._use_mlflow = use_mlflow\n",
    "        self._best_acc = -1\n",
    "\n",
    "        # Model Skeleton\n",
    "        # train from scratch: no pretrained weights\n",
    "        # models.efficientnet_b0(num_classes=self._num_classes, weights=None)\n",
    "        # Here we use pretrained weights as it's too costly and slow to train this, no num classes needed\n",
    "        self._model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # IMAGENET classifier\n",
    "        # Sequential((0): Dropout(p=0.2, inplace=True)\n",
    "        #            (1): Linear(in_features=1280, out_features=1000, bias=True))\n",
    "        # replace to 100 classes to fit CIFAR100\n",
    "        self._model.classifier[1] = nn.Linear(1280, 100)\n",
    "        self._device = self._set_device()\n",
    "        # set it to device setup; by default, model weights are on CPU, and input data is on MPS device\n",
    "        self._model.to(self._device)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "        # Hyperparameters\n",
    "        self._batch_size = batch_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "        # Loss function & Optimizer\n",
    "        self._criterion = self._set_criterion()\n",
    "        self._optimizer = self._set_optimizer()\n",
    "\n",
    "    def _set_device(self):\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def _set_optimizer(self):\n",
    "        return optim.Adam(self._model.parameters(), lr=self._learning_rate)\n",
    "\n",
    "    def _set_criterion(self):\n",
    "        return nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Define computation in forward(), call via model(X), pytorch auto calls model.forward(X)\n",
    "        Return:\n",
    "            Logits vector, the inference results\n",
    "        \"\"\"\n",
    "        return self.model(X)\n",
    "\n",
    "    # Train Epoch for 1 Loop\n",
    "    def train_epoch(self, data_loader, epoch_idx):\n",
    "        \"\"\"\n",
    "        1. Prediction: tensor([20, 58, 76, 61, 44, 44, 92, 60, 51, 87, 18, 51, 60, 87, 12, 75, 64, 10, 63, 26, 64, 70, 76, 66, 81, 20, 90, 76, 93, 42, 23, 13], device='mps:0')\n",
    "        Original tensor: [32, 100], 32 samples in one batch, 100 classes in each sample\n",
    "        2. Mask: tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], device='mps:0')\n",
    "        3. Mask tensor is boolean tenser in [32, 100], True/False\n",
    "        4. Use argmax column direction, we get [32, 1], the index of the largest probability in each sample\n",
    "        5. All the prediction, targets, loss, are tensor objects\n",
    "\n",
    "        Args:\n",
    "            loader: pytorch DataLoader instance for image iteration\n",
    "        Return:\n",
    "            epoch_loss: the average loss for current epoch: total loss / total batches\n",
    "            epoch_acc: the average accuracy for current epoch: total correct prediction / total true labels\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting epoch {epoch_idx+1}, total batches: {len(data_loader)}\")\n",
    "        # print(f\"Starting epoch {epoch_idx+1}, total batches: {len(data_loader)}\")\n",
    "\n",
    "        # Set model mode to train, so the Dropout, BatchNorm are up to work\n",
    "        self._model.train()\n",
    "        epoch_total_loss = 0.0  # Track full epoch\n",
    "        running_loss = 0.0      # Track 10-batch average\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "\n",
    "        for idx, (images, targets) in enumerate(data_loader):\n",
    "            images = images.to(self._device)\n",
    "            # labels represented in int, not one-hot encoding\n",
    "            targets = targets.to(self._device)\n",
    "            # Clears(Reset) accumulated gradients from the previous iteration\n",
    "            self._optimizer.zero_grad()\n",
    "            # inference result from the model, it will auto call forward(images)\n",
    "            logits = self._model(images)\n",
    "            # Calculate the loss, compare Inference Prediction vs. True Labels\n",
    "            loss = self._criterion(logits, targets)\n",
    "            # backtrack to update the gradients\n",
    "            loss.backward()\n",
    "            # Update weights and parameters using latest gradients from backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            predictions = logits.argmax(dim=1)\n",
    "\n",
    "            # how many predictions match true labels\n",
    "            running_correct += (predictions == targets).sum().item()\n",
    "            # total number of the true labels\n",
    "            running_total += targets.size(0)\n",
    "            running_loss += loss.item()\n",
    "            epoch_total_loss += loss.item()\n",
    "\n",
    "            # print out progress every 10 batches\n",
    "            if idx % 10 == 9:\n",
    "                avg_loss = running_loss / 10\n",
    "                curr_acc = running_correct / running_total\n",
    "                logger.info(f\"Epoch {epoch_idx + 1}| Batch {idx + 1} | Avg Loss {avg_loss: .4f} | Accuracy: {curr_acc:.4f}\")\n",
    "                running_loss = 0.0      # reset for every 10 batches\n",
    "\n",
    "        # return loss and accuracy for current training epoch\n",
    "        epoch_loss = epoch_total_loss / len(data_loader)     # total loss / total batches\n",
    "        epoch_acc = running_correct / running_total     # total correct prediction / total true labels\n",
    "\n",
    "        logger.info(f\"Epoch loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    # MLFlow Setup\n",
    "    def _setup_mlflow(self):\n",
    "        \"\"\"Initialize Mlflow experiment and log parameters\"\"\"\n",
    "        mlflow.set_tracking_uri(\"http://127.0.0.1:5555\")\n",
    "        # setup name of experiment\n",
    "        mlflow.set_experiment(\"EfficientNet-B0-CIFAR100\")\n",
    "        # this will start a global state for all the mflow.log_*()\n",
    "        mlflow.start_run(run_name=datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "        mlflow.log_params({\n",
    "            \"model\": \"EfficientNet-B0\",\n",
    "            \"dataset\": \"CIFAR-100\",\n",
    "            \"batch_size\": self._batch_size,\n",
    "            \"learning_rate\": self._learning_rate,\n",
    "            \"epochs\": self._num_epochs,\n",
    "            \"device\": self._device.type,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"num_classes\": self._num_classes\n",
    "        })\n",
    "\n",
    "    def _log_metrics(self, epoch, epoch_loss, epoch_acc):\n",
    "        \"\"\"Log metrics to Mlflow\"\"\"\n",
    "        if not self._use_mlflow:\n",
    "            return\n",
    "\n",
    "        metrics = {\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"train_accuracy\": epoch_acc,\n",
    "            \"learning_rate\": self._optimizer.param_groups[0][\"lr\"]\n",
    "        }\n",
    "\n",
    "        mlflow.log_metrics(metrics, step=epoch)\n",
    "\n",
    "    def _log_best_model(self, epoch_acc):\n",
    "        \"\"\"Log model if it's the best so far\"\"\"\n",
    "        if not self._use_mlflow:\n",
    "            return\n",
    "\n",
    "        # Only log current model if the accuracy is better\n",
    "        if epoch_acc <= self._best_acc:\n",
    "            return\n",
    "        # update to better accuracy\n",
    "        self._best_acc = epoch_acc\n",
    "\n",
    "        # TODO: understand this\n",
    "        sample_input = torch.randn(1, 3, 224, 224).to(self._device)\n",
    "        signature = infer_signature(\n",
    "                sample_input.cpu().numpy(),\n",
    "                self._model(sample_input).detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "        # Log the model, the first one will always log, then keep overriding for better ones\n",
    "        mlflow.pytorch.log_model(\n",
    "            self._model,\n",
    "            artifact_path=\"best_model\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        \"\"\"Main training loop with Mlflow integration\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Training {self._num_epochs} epochs on {self._device}\")\n",
    "            # init mlflow, new run & experiment\n",
    "            if self._use_mlflow:\n",
    "                self._setup_mlflow()\n",
    "\n",
    "            for epoch in range(self._num_epochs):\n",
    "                epoch_loss, epoch_acc = self.train_epoch(train_loader, epoch)\n",
    "                logger.info(f\"Epoch {epoch + 1}/{self._num_epochs}: Avg Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "                # log metrics to mlflow\n",
    "                if self._use_mlflow:\n",
    "                    self._log_metrics(epoch, epoch_loss, epoch_acc)\n",
    "\n",
    "                # update current best model to gcs\n",
    "                self._log_best_model(epoch_acc)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Train process failed, error: {e}\")\n",
    "        finally:\n",
    "            if self._use_mlflow:\n",
    "                logger.debug(\"Mlflow: Ending current run...\")\n",
    "                mlflow.end_run()\n",
    "                logger.debug(\"Mlflow: current run ended successfully\")\n"
   ],
   "id": "23dae7e494455dfc",
   "outputs": [],
   "execution_count": 179
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-23T05:01:23.296324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = MyEfficientNet(num_epochs=15)\n",
    "m.train(train_loader)\n",
    "\n",
    "# TODO: frontend shows batchsize = 32, incorrect, fix it"
   ],
   "id": "30652f01ff5c0e95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 22:01:23,460 - INFO - 2456644747.py - PID:61797 - TID:8386830080 - Training 15 epochs on mps\n",
      "2025-06-22 22:01:23,500 - INFO - 2456644747.py - PID:61797 - TID:8386830080 - Starting epoch 1, total batches: 1250\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd0f70bed584cf66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
