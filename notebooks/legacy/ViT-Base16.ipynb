{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uQKGFrVPIkni",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23308,
     "status": "ok",
     "timestamp": 1751964023185,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "uQKGFrVPIkni",
    "outputId": "a4b996a8-4408-4cf5-da5c-52740e7531e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==3.1.1 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
      "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.0)\n",
      "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
      "Downloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gunicorn, graphql-core, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
      "Successfully installed alembic-1.16.2 databricks-sdk-0.57.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.1 mlflow-skinny-3.1.1 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import platform\n",
    "import psutil\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "PvSSOAr9Ikll",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751964023191,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "PvSSOAr9Ikll"
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "format = '%(asctime)s - %(levelname)s - %(filename)s - PID:%(process)d - TID:%(thread)d - %(message)s'\n",
    "logger = logging.getLogger(__name__ + str(time.time()))\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "# Add handler if none exists\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(format))\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "JtAHkBXCIkjW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142536,
     "status": "ok",
     "timestamp": 1751964165729,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "JtAHkBXCIkjW",
    "outputId": "8e6a32f3-8ecb-4d1c-91ab-a26709db89d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [02:18<00:00, 1.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 50000\n",
      "Number of classes: 100\n",
      "Class names (first 10): ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle']\n"
     ]
    }
   ],
   "source": [
    "# Data transforms for CIFAR-100 - ViT requires 224x224 images\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(224),             # Resize to 224x224 for ViT\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),              # Convert to tensor\n",
    "    transforms.Normalize(               # Normalize with ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_ds = datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_tf\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_ds)}\")\n",
    "print(f\"Number of classes: {len(train_ds.classes)}\")\n",
    "print(f\"Class names (first 10): {train_ds.classes[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cYmjoFFfIkhH",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751964165734,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "cYmjoFFfIkhH"
   },
   "outputs": [],
   "source": [
    "# Optional: Visualize a sample of the data\n",
    "def visualize_samples(dataset, num_samples=16):\n",
    "    \"\"\"Visualize a sample of images from the dataset\"\"\"\n",
    "    # Create subset for visualization\n",
    "    sample_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        img_tensor, class_idx = dataset[idx]\n",
    "\n",
    "        # Denormalize image for display\n",
    "        img = img_tensor.permute(1, 2, 0)\n",
    "        img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Class: {dataset.classes[class_idx]}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to visualize samples\n",
    "# visualize_samples(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afPNIG5bIke8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751964977491,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "afPNIG5bIke8",
    "outputId": "2073ad7b-b2a9-4ddc-cfce-58112aef5aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced batches per epoch: 79\n",
      "Number of batches per epoch: 79\n",
      "Total samples per epoch: 10112\n"
     ]
    }
   ],
   "source": [
    "# Create subset of 10,000 samples\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Random subset of 10,000 training samples\n",
    "subset_indices = np.random.choice(len(train_ds), 10000, replace=False)\n",
    "train_subset = Subset(train_ds, subset_indices)\n",
    "\n",
    "# Create dataloader with subset\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=128,\n",
    "    shuffle=True)\n",
    "print(f\"Reduced batches per epoch: {len(train_loader)}\")  # ~78 batches\n",
    "\n",
    "\n",
    "# # Create data loader\n",
    "# train_loader = DataLoader(\n",
    "#     train_ds,\n",
    "#     batch_size=128,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "print(f\"Number of batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Total samples per epoch: {len(train_loader) * 128}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saQ9unOIIkcw",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1751964982838,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "saQ9unOIIkcw"
   },
   "outputs": [],
   "source": [
    "class ComprehensiveTrainingMonitor:\n",
    "    \"\"\"Enhanced training monitor with MLflow integration and comprehensive metrics\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion, device, model_name='ViT-Base16',\n",
    "                 dataset_name='CIFAR-100', batch_size=128, epochs=100,\n",
    "                 input_size=(3, 224, 224), use_mlflow=True,\n",
    "                 learning_rate=5e-3, use_pretrained=True, train_size=50000,\n",
    "                 val_size=10000, num_workers=0):\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.input_size = input_size\n",
    "        self.use_mlflow = use_mlflow\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Tracking variables\n",
    "        self.best_metric = float('-inf')\n",
    "        self.epoch_times = []\n",
    "        self.start_time = time.time()\n",
    "        self.run_started = False\n",
    "\n",
    "        # MLflow configuration\n",
    "        self.mlflow_uri = \"https://neuralripper.com/mlflow/\"\n",
    "        self.gcs_bucket = \"gs://neuralripper-mlflow-artifacts\"\n",
    "\n",
    "        if self.use_mlflow:\n",
    "            self._initialize_mlflow()\n",
    "\n",
    "    def _initialize_mlflow(self):\n",
    "        \"\"\"Initialize MLflow with comprehensive experiment tracking\"\"\"\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(self.mlflow_uri)\n",
    "            mlflow.set_experiment(f\"{self.model_name}-{self.dataset_name}\")\n",
    "\n",
    "            # Start run with timestamp\n",
    "            run_name = f\"{self.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            mlflow.start_run(run_name=run_name)\n",
    "            self.run_started = True\n",
    "\n",
    "            # Log all parameters\n",
    "            params = {\n",
    "                **self._get_model_params(),\n",
    "                **self._get_system_params(),\n",
    "                **self._get_environment_params(),\n",
    "                **self._get_data_params(),\n",
    "                **self._get_training_params(),\n",
    "            }\n",
    "\n",
    "            mlflow.log_params(params)\n",
    "            logger.info(f\"MLflow run started: {run_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to initialize MLflow: {e}\")\n",
    "            self.use_mlflow = False\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Model architecture and hyperparameters\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"total_parameters\": total_params,\n",
    "            \"trainable_parameters\": trainable_params,\n",
    "            \"model_type\": \"classification\",\n",
    "            \"pretrained\": self.use_pretrained,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"optimizer\": self.optimizer.__class__.__name__,\n",
    "            \"criterion\": self.criterion.__class__.__name__,\n",
    "            \"input_channels\": self.input_size[0],\n",
    "            \"input_height\": self.input_size[1],\n",
    "            \"input_width\": self.input_size[2],\n",
    "        }\n",
    "\n",
    "    def _get_system_params(self):\n",
    "        \"\"\"System hardware and software parameters\"\"\"\n",
    "        gpu_info = {}\n",
    "        if torch.cuda.is_available():\n",
    "            props = torch.cuda.get_device_properties(0)\n",
    "            gpu_info = {\n",
    "                \"gpu_name\": torch.cuda.get_device_name(0),\n",
    "                \"gpu_memory_gb\": round(props.total_memory / (1024**3), 2),\n",
    "                \"cuda_version\": torch.version.cuda,\n",
    "                \"num_gpus\": torch.cuda.device_count(),\n",
    "            }\n",
    "        elif torch.backends.mps.is_available():\n",
    "            gpu_info = {\n",
    "                \"gpu_name\": \"Apple Silicon MPS\",\n",
    "                \"device_type\": \"mps\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"cpu_count\": psutil.cpu_count(),\n",
    "            \"memory_total_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "            \"platform\": platform.platform(),\n",
    "            \"python_version\": platform.python_version(),\n",
    "            \"pytorch_version\": torch.__version__,\n",
    "            **gpu_info\n",
    "        }\n",
    "\n",
    "    def _get_environment_params(self):\n",
    "        \"\"\"Environment and reproducibility parameters\"\"\"\n",
    "        git_info = {}\n",
    "        try:\n",
    "            commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()\n",
    "            branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode().strip()\n",
    "            git_info = {\n",
    "                \"git_commit\": commit[:8],\n",
    "                \"git_branch\": branch,\n",
    "            }\n",
    "        except:\n",
    "            git_info = {\"git_commit\": \"unknown\", \"git_branch\": \"unknown\"}\n",
    "\n",
    "        return git_info\n",
    "\n",
    "    def _get_data_params(self):\n",
    "        \"\"\"Data pipeline parameters\"\"\"\n",
    "        return {\n",
    "            \"train_size\": self.train_size,\n",
    "            \"val_size\": self.val_size,\n",
    "            \"total_samples\": self.train_size + self.val_size,\n",
    "            \"num_workers\": self.num_workers,\n",
    "        }\n",
    "\n",
    "    def _get_training_params(self):\n",
    "        \"\"\"Advanced training configuration\"\"\"\n",
    "        return {\n",
    "            \"mlflow_uri\": self.mlflow_uri,\n",
    "            \"experiment_name\": f\"{self.model_name}-{self.dataset_name}\",\n",
    "        }\n",
    "\n",
    "    def log_epoch_metrics(self, epoch, epoch_loss, epoch_acc, batch_count=None):\n",
    "        \"\"\"Comprehensive epoch metrics logging\"\"\"\n",
    "        if not self.use_mlflow or not self.run_started:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            epoch_time = self.epoch_times[-1] if self.epoch_times else 0\n",
    "\n",
    "            # Core metrics\n",
    "            metrics = {\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"train_accuracy\": epoch_acc,\n",
    "                \"epoch_time_seconds\": epoch_time,\n",
    "                \"learning_rate\": self.optimizer.param_groups[0]['lr'],\n",
    "            }\n",
    "\n",
    "            # Performance metrics\n",
    "            if batch_count:\n",
    "                metrics[\"batches_per_second\"] = batch_count / epoch_time if epoch_time > 0 else 0\n",
    "                metrics[\"samples_per_second\"] = (batch_count * self.batch_size) / epoch_time if epoch_time > 0 else 0\n",
    "\n",
    "            # Memory metrics\n",
    "            if torch.cuda.is_available():\n",
    "                metrics[\"gpu_memory_allocated_gb\"] = torch.cuda.memory_allocated() / (1024**3)\n",
    "                metrics[\"gpu_memory_reserved_gb\"] = torch.cuda.memory_reserved() / (1024**3)\n",
    "\n",
    "            # Running statistics\n",
    "            total_time = sum(self.epoch_times)\n",
    "            metrics[\"total_time_minutes\"] = total_time / 60\n",
    "            metrics[\"average_epoch_time\"] = np.mean(self.epoch_times) if self.epoch_times else 0\n",
    "\n",
    "            mlflow.log_metrics(metrics, step=epoch)\n",
    "\n",
    "            # Update best model if improved\n",
    "            if epoch_acc > self.best_metric:\n",
    "                self.best_metric = epoch_acc\n",
    "                self._log_model_checkpoint(epoch)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log metrics for epoch {epoch}: {e}\")\n",
    "\n",
    "    def _log_model_checkpoint(self, epoch):\n",
    "        \"\"\"Log model checkpoint and metadata\"\"\"\n",
    "        try:\n",
    "            # Log the model\n",
    "            mlflow.pytorch.log_model(\n",
    "                self.model,\n",
    "                artifact_path=\"model\",\n",
    "                registered_model_name=f\"{self.model_name}-{self.dataset_name}\",\n",
    "                pip_requirements=[\"torch\", \"torchvision\", \"pillow\", \"numpy\"]\n",
    "            )\n",
    "\n",
    "            # Log additional metadata\n",
    "            model_metadata = {\n",
    "                \"best_epoch\": epoch,\n",
    "                \"best_accuracy\": self.best_metric,\n",
    "                \"checkpoint_time\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "            mlflow.log_params(model_metadata)\n",
    "            logger.info(f\"Model metadata logged for accuracy: {self.best_metric:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log model metadata: {e}\")\n",
    "\n",
    "    def end_run(self, status=\"FINISHED\"):\n",
    "        \"\"\"Clean up and end MLflow run\"\"\"\n",
    "        if self.use_mlflow and self.run_started:\n",
    "            try:\n",
    "                total_time = time.time() - self.start_time\n",
    "                summary = {\n",
    "                    \"final_total_training_time_minutes\": round(total_time / 60, 2),\n",
    "                    \"final_best_accuracy\": self.best_metric,\n",
    "                    \"final_epochs_completed\": len(self.epoch_times),\n",
    "                }\n",
    "                mlflow.log_params(summary)\n",
    "                mlflow.end_run(status=status)\n",
    "                self.run_started = False\n",
    "                return summary\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to end MLflow run properly: {e}\")\n",
    "                return {}\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bj7gLkwOIkac",
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1751964983922,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "bj7gLkwOIkac"
   },
   "outputs": [],
   "source": [
    "class EnhancedViTBase16:\n",
    "    \"\"\"ViT-Base16 with comprehensive monitoring integration\"\"\"\n",
    "\n",
    "    def __init__(self, num_epochs=100, batch_size=128, num_classes=100,\n",
    "                 learning_rate=5e-3, use_mlflow=True):\n",
    "        # Core parameters\n",
    "        self._num_classes = num_classes\n",
    "        self._use_mlflow = use_mlflow\n",
    "        self._batch_size = batch_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "        # Create model components\n",
    "        self._model = self._create_model()\n",
    "        self._device = self._set_device()\n",
    "        self._model.to(self._device)\n",
    "        self._criterion = self._set_criterion()\n",
    "        self._optimizer = self._set_optimizer()\n",
    "\n",
    "        # Initialize comprehensive monitor\n",
    "        self.monitor = ComprehensiveTrainingMonitor(\n",
    "            model=self._model,\n",
    "            optimizer=self._optimizer,\n",
    "            criterion=self._criterion,\n",
    "            device=self._device,\n",
    "            model_name='ViT-Base16',\n",
    "            dataset_name='CIFAR-100',\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            input_size=(3, 224, 224),  # ViT uses 224x224\n",
    "            learning_rate=learning_rate,\n",
    "            use_mlflow=use_mlflow,\n",
    "            use_pretrained=True,\n",
    "            train_size=50000,\n",
    "            val_size=10000,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Model initialized on device: {self._device}\")\n",
    "        logger.info(f\"Model parameters: {sum(p.numel() for p in self._model.parameters()):,}\")\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create ViT-Base16 model with CIFAR-100 adaptation\"\"\"\n",
    "        model = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        # Adapt classifier for CIFAR-100 (100 classes)\n",
    "        num_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(num_features, self._num_classes)\n",
    "        return model\n",
    "\n",
    "    def _set_device(self):\n",
    "        \"\"\"Set appropriate device for training\"\"\"\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    def _set_optimizer(self):\n",
    "        \"\"\"Configure SGD optimizer with momentum and weight decay\"\"\"\n",
    "        return optim.SGD(self._model.parameters(),\n",
    "                        lr=self._learning_rate,\n",
    "                        momentum=0.9,\n",
    "                        weight_decay=4e-5)\n",
    "\n",
    "    def _set_criterion(self):\n",
    "        \"\"\"Set loss function for classification\"\"\"\n",
    "        return nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_epoch(self, data_loader, epoch_idx):\n",
    "        \"\"\"Enhanced training epoch with comprehensive monitoring\"\"\"\n",
    "        logger.info(f\"Starting epoch {epoch_idx+1}, total batches: {len(data_loader)}\")\n",
    "\n",
    "        self._model.train()\n",
    "        epoch_total_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        batch_count = len(data_loader)\n",
    "\n",
    "        for idx, (images, targets) in enumerate(data_loader):\n",
    "            images = images.to(self._device)\n",
    "            targets = targets.to(self._device)\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            logits = self._model(images)\n",
    "            loss = self._criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            running_correct += (predictions == targets).sum().item()\n",
    "            running_total += targets.size(0)\n",
    "            running_loss += loss.item()\n",
    "            epoch_total_loss += loss.item()\n",
    "\n",
    "            # Log progress every 10 batches\n",
    "            if idx % 10 == 9:\n",
    "                avg_loss = running_loss / 10\n",
    "                acc_sofar = running_correct / running_total\n",
    "                logger.info(f\"Epoch {epoch_idx+1} | Batch {idx+1}/{batch_count} | \"\n",
    "                          f\"Loss {avg_loss:.4f} | Acc {acc_sofar:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = epoch_total_loss / batch_count\n",
    "        epoch_acc = running_correct / running_total\n",
    "\n",
    "        logger.info(f\"Epoch {epoch_idx+1} completed | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"Full training loop with comprehensive monitoring\"\"\"\n",
    "        logger.info(\"Starting training process\")\n",
    "\n",
    "        try:\n",
    "            for epoch in range(self._num_epochs):\n",
    "                epoch_start = time.time()\n",
    "\n",
    "                # Train for one epoch\n",
    "                epoch_loss, epoch_acc = self.train_epoch(data_loader, epoch)\n",
    "\n",
    "                # Track timing\n",
    "                epoch_time = time.time() - epoch_start\n",
    "                self.monitor.epoch_times.append(epoch_time)\n",
    "\n",
    "                # Log metrics\n",
    "                self.monitor.log_epoch_metrics(epoch, epoch_loss, epoch_acc, len(data_loader))\n",
    "\n",
    "                # Progress report\n",
    "                eta = np.mean(self.monitor.epoch_times) * (self._num_epochs - epoch - 1)\n",
    "                logger.info(f\"Epoch {epoch+1}/{self._num_epochs} | \"\n",
    "                          f\"Time: {epoch_time:.1f}s | ETA: {eta/60:.1f}min\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Training interrupted by user\")\n",
    "            summary = self.monitor.end_run(status=\"KILLED\")\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed with error: {e}\")\n",
    "            summary = self.monitor.end_run(status=\"FAILED\")\n",
    "            raise e\n",
    "\n",
    "        # Training completed\n",
    "        summary = self.monitor.end_run(status=\"FINISHED\")\n",
    "        logger.info(f\"Training completed. Summary: {summary}\")\n",
    "\n",
    "        if summary:\n",
    "            print(f\"\\nTraining Summary:\")\n",
    "            print(f\"Total time: {summary.get('final_total_training_time_minutes', 0):.1f} minutes\")\n",
    "            print(f\"Best accuracy: {summary.get('final_best_accuracy', 0):.4f}\")\n",
    "            print(f\"Epochs completed: {summary.get('final_epochs_completed', 0)}\")\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "H0WzGBa8IkYb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1751964988428,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "H0WzGBa8IkYb",
    "outputId": "bd2130b0-5a81-45dc-ed0b-c57d80d562d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 08:56:26,605 - INFO - ipython-input-15-2648212344.py - PID:3672 - TID:133238535799424 - Initializing Enhanced ViT-Base16 model\n",
      "2025-07-08 08:56:28,344 - WARNING - ipython-input-13-2247694904.py - PID:3672 - TID:133238535799424 - Failed to initialize MLflow: Run with UUID d637189b65554bc6b1550801df275bbb is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n",
      "2025-07-08 08:56:28,345 - INFO - ipython-input-14-184517597.py - PID:3672 - TID:133238535799424 - Model initialized on device: cuda\n",
      "2025-07-08 08:56:28,346 - INFO - ipython-input-14-184517597.py - PID:3672 - TID:133238535799424 - Model parameters: 85,875,556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n",
      "Device: cuda\n",
      "Total parameters: 85,875,556\n",
      "Trainable parameters: 85,875,556\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "logger.info(\"Initializing Enhanced ViT-Base16 model\")\n",
    "\n",
    "model = EnhancedViTBase16(\n",
    "    num_epochs=10,\n",
    "    batch_size=256,\n",
    "    num_classes=100,\n",
    "    learning_rate=5e-3,\n",
    "    use_mlflow=True\n",
    ")\n",
    "\n",
    "print(\"Model initialized successfully!\")\n",
    "print(f\"Device: {model._device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model._model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model._model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "un9H-pesIkWS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1422210,
     "status": "ok",
     "timestamp": 1751966413438,
     "user": {
      "displayName": "DizzyDoze",
      "userId": "13303002129287970688"
     },
     "user_tz": 420
    },
    "id": "un9H-pesIkWS",
    "outputId": "145dd076-4b57-4cbc-b34f-02b397f25434"
   },
   "outputs": [],
   "source": [
    "# Start training with comprehensive monitoring\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" STARTING ENHANCED VIT-BASE16 TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: CIFAR-100 (100 classes)\")\n",
    "print(f\"Model: ViT-Base16 (pretrained)\")\n",
    "print(f\"Epochs: {model._num_epochs}\")\n",
    "print(f\"Batch size: {model._batch_size}\")\n",
    "print(f\"Learning rate: {model._learning_rate}\")\n",
    "print(f\"MLflow tracking: {'Enabled' if model._use_mlflow else 'Disabled'}\")\n",
    "print(f\"MLflow URI: {model.monitor.mlflow_uri}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Start training\n",
    "model.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VBnDmofSIkUJ",
   "metadata": {
    "id": "VBnDmofSIkUJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LnUnAfalIkSC",
   "metadata": {
    "id": "LnUnAfalIkSC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WGGUYHpPIkP-",
   "metadata": {
    "id": "WGGUYHpPIkP-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235093af5f76886",
   "metadata": {
    "id": "a235093af5f76886"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53e09cc9ba1308",
   "metadata": {
    "id": "ab53e09cc9ba1308"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141852556539e3b",
   "metadata": {
    "id": "b141852556539e3b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
